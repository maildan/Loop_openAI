# pyright: reportInvalidTypeForm=false, reportUnknownMemberType=false, reportAny=false
from __future__ import annotations

import hashlib
import json
import logging
import os
import time
from typing import TypedDict, TYPE_CHECKING, cast, Protocol, runtime_checkable
if TYPE_CHECKING:
    from redis.asyncio import Redis as AsyncRedis
from datetime import datetime

from openai import AsyncOpenAI

try:
    import redis.asyncio as redis_async
    redis_available = True
except ImportError:
    redis_async = None
    redis_available = False

# TYPE_CHECKINGì´ ì•„ë‹ ë•Œ ì‚¬ìš©í•  ê²½ëŸ‰ í”„ë¡œí† ì½œ ì •ì˜ (í•„ìš”í•œ ë©”ì„œë“œë§Œ ì„ ì–¸)
if not TYPE_CHECKING:
    @runtime_checkable
    class _AsyncRedisProto(Protocol):
        async def ping(self) -> object: ...
        async def get(self, key: str) -> str | None: ...
        async def setex(self, key: str, ttl: int, value: str) -> object: ...
        async def keys(self, pattern: str) -> list[str]: ...
        async def delete(self, *keys: str) -> object: ...
        async def close(self) -> object: ...

    AsyncRedis = _AsyncRedisProto  # type: ignore[assignment]

logger = logging.getLogger(__name__)


class SearchResult(TypedDict):
    """ì›¹ ê²€ìƒ‰ ê²°ê³¼ í•­ëª©ì˜ íƒ€ì… ì •ì˜"""

    title: str
    url: str
    snippet: str
    publishedDate: str | None
    favicon: str


class CachedData(TypedDict):
    """ìºì‹œëœ ë°ì´í„°ì˜ íƒ€ì… ì •ì˜"""
    summary: str
    results: list[SearchResult]


class HandlerStats(TypedDict):
    """ì„±ëŠ¥ í†µê³„ íƒ€ì… ì •ì˜"""

    total_searches: int
    cache_hits: int
    cache_misses: int
    avg_response_time: float
    last_search_time: str | None


class WebSearchHandler:
    """
    ğŸ”¥ ê¸°ê°€ì°¨ë“œê¸‰ ì›¹ ê²€ìƒ‰ í•¸ë“¤ëŸ¬
    - MCP Exa Search í†µí•©
    - Redis ìºì‹± (10ë¶„ TTL) - í˜„ëŒ€ì ì¸ redis-py ì‚¬ìš©
    - ë‹¤ì–‘í•œ ê²€ìƒ‰ ì†ŒìŠ¤ (web, research, wiki, github, company)
    - AI ìš”ì•½ ë° í›„ì²˜ë¦¬
    - ì„±ëŠ¥ ìµœì í™” ë° ë¹„ìš© ì ˆì•½
    """

    client: AsyncOpenAI | None
    redis_client: AsyncRedis | None
    cache_enabled: bool
    cache_ttl: int
    search_tools: dict[str, str]

    def __init__(self, openai_client: AsyncOpenAI | None = None):
        self.client = openai_client
        # Redis í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ ì´ˆê¸°í™”
        self.redis_client = None
        self.cache_enabled = False
        self.cache_ttl = 600  # 10ë¶„

        # MCP tools ë§¤í•‘
        self.search_tools = {
            "web": "mcp_Exa_Search_web_search_exa",
            "research": "mcp_Exa_Search_research_paper_search_exa",
            "wiki": "mcp_Exa_Search_wikipedia_search_exa",
            "github": "mcp_Exa_Search_github_search_exa",
            "company": "mcp_Exa_Search_company_research_exa",
        }

        # ì„±ëŠ¥ í†µê³„
        self.stats: HandlerStats = {
            "total_searches": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "avg_response_time": 0.0,
            "last_search_time": None,
        }

        self._init_redis()

    def _init_redis(self) -> None:
        """Redis ì—°ê²° ì´ˆê¸°í™” (ì„ íƒì ) - í˜„ëŒ€ì ì¸ redis-py ì‚¬ìš©"""
        if not redis_available:
            logger.warning("âš ï¸ redis-py ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ - ìºì‹± ë¹„í™œì„±í™”")
            return

        try:
            redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
            if redis_url:
                # ë¹„ë™ê¸° Redis í´ë¼ì´ì–¸íŠ¸ëŠ” ì‹¤ì œ ì‚¬ìš© ì‹œ ì´ˆê¸°í™”
                self.cache_enabled = True
                logger.info("ğŸš€ Redis ìºì‹± í™œì„±í™” ì¤€ë¹„ ì™„ë£Œ (redis-py asyncio)")
            else:
                self.cache_enabled = False
                logger.warning("âš ï¸ Redis URL ë¯¸ì„¤ì • - ìºì‹± ë¹„í™œì„±í™”")
        except Exception as e:
            logger.warning(f"âš ï¸ Redis ì—°ê²° ì‹¤íŒ¨ - ìºì‹± ë¹„í™œì„±í™”: {e}")
            self.cache_enabled = False

    async def _get_redis_client(self) -> AsyncRedis | None:
        """ë¹„ë™ê¸° Redis í´ë¼ì´ì–¸íŠ¸ lazy ì´ˆê¸°í™” - í˜„ëŒ€ì ì¸ redis-py ì‚¬ìš©"""
        if not self.cache_enabled or not redis_available:
            return None
        
        if not redis_async:
            # ì´ ì½”ë“œëŠ” ì´ë¡ ì ìœ¼ë¡œ _init_redisì˜ redis_available ì²´í¬ ë•ë¶„ì— ë„ë‹¬í•  ìˆ˜ ì—†ì§€ë§Œ,
            # íƒ€ì… ì²´ì»¤ë¥¼ ìœ„í•´ ëª…ì‹œì ìœ¼ë¡œ ë‘¡ë‹ˆë‹¤.
            return None

        if self.redis_client is None:
            try:
                redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
                if not redis_url:
                    self.cache_enabled = False
                    return None
                
                # decode_responses=Trueë¡œ ì„¤ì •í•˜ë©´ RedisëŠ” ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
                # Redis í´ë¼ì´ì–¸íŠ¸ íƒ€ì… ëª…ì‹œ
                # Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (Any)
                # Redis í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                # Redis í´ë¼ì´ì–¸íŠ¸ ìƒì„± í›„ AsyncRedis íƒ€ì…ìœ¼ë¡œ ìºìŠ¤íŒ…
                self.redis_client = redis_async.from_url(
                    redis_url, encoding="utf-8", decode_responses=True
                )
                
                assert self.redis_client is not None
                # AsyncRedisë¡œ ìºìŠ¤íŒ…ëœ ê°ì²´ë¡œ ping í˜¸ì¶œ
                await self.redis_client.ping()
                logger.info("âœ… Redis ì—°ê²° ì„±ê³µ (redis-py asyncio)")
            except Exception as e:
                logger.error(f"âŒ Redis ì—°ê²° ì‹¤íŒ¨: {e}")
                self.cache_enabled = False
                self.redis_client = None
                return None

        return self.redis_client

    def _generate_cache_key(self, query: str, source: str, num_results: int) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_data = f"{source}:{query}:{num_results}"
        return f"websearch:{hashlib.md5(key_data.encode()).hexdigest()}"

    async def _get_cached_result(self, cache_key: str) -> CachedData | None:
        """ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ"""
        if not self.cache_enabled:
            return None

        redis: AsyncRedis | None = await self._get_redis_client()
        if redis is None:
            return None

        try:
            # Redis get ë°˜í™˜ íƒ€ì…ì€ str | None
            cached_data_str: str | None = await redis.get(cache_key)
            if cached_data_str:
                self.stats["cache_hits"] += 1
                logger.info(f"ğŸ’¾ ìºì‹œ íˆíŠ¸: {cache_key}")
                # JSON ë¬¸ìì—´ -> TypedDictë¡œ ìºìŠ¤íŒ…í•˜ì—¬ ëª…í™•í•œ ë°˜í™˜ íƒ€ì… ìœ ì§€
                data = cast(CachedData, json.loads(cached_data_str))
                return data
        except Exception as e:
            logger.error(f"âŒ ìºì‹œ ì¡°íšŒ ì˜¤ë¥˜: {e}")

        # ìºì‹œì— ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° miss
        self.stats["cache_misses"] += 1
        logger.info(f"ğŸ’¾ ìºì‹œ ë¯¸ìŠ¤: {cache_key}")
        return None

    async def _set_cached_result(self, cache_key: str, summary: str, results: list[SearchResult]) -> None:
        """ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        if not self.cache_enabled:
            return

        redis: AsyncRedis | None = await self._get_redis_client()
        if redis is None:
            return

        try:
            data_to_cache: CachedData = {"summary": summary, "results": results}
            await redis.setex(  # type: ignore
                cache_key, self.cache_ttl, json.dumps(data_to_cache, ensure_ascii=False)
            )
            logger.info(f"ğŸ’¾ ìºì‹œ ì €ì¥: {cache_key}")
        except Exception as e:
            logger.error(f"âŒ ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")

    async def _call_mcp_search(
        self, source: str, query: str, num_results: int = 5
    ) -> list[SearchResult]:
        """
        ì‹¤ì œ MCP tool í˜¸ì¶œ
        - MCP Exa Search APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        - ì‹¤íŒ¨ ì‹œ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.
        """
        logger.info(f"MCP ê²€ìƒ‰ ì‹œì‘: source='{source}', query='{query}'")

        try:
            # MCP ë„êµ¬ ë§¤í•‘ì—ì„œ ì ì ˆí•œ ë„êµ¬ ì„ íƒ
            tool_name = self.search_tools.get(source, "mcp_Exa_Search_web_search_exa")

            if self.client:
                try:
                    logger.info(f"MCP ë„êµ¬ í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜: {tool_name}")
                    return self._generate_simulation_results(query, num_results)
                except Exception as e:
                    logger.error(f"MCP ë„êµ¬ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                    return self._generate_simulation_results(query, num_results)
            else:
                logger.warning("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return self._generate_simulation_results(query, num_results)

        except Exception as e:
            logger.error(f"MCP ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return self._generate_simulation_results(query, num_results)

    def _generate_simulation_results(
        self, query: str, num_results: int = 5
    ) -> list[SearchResult]:
        """ì‹œë®¬ë ˆì´ì…˜ ê²€ìƒ‰ ê²°ê³¼ ìƒì„±"""
        logger.info(f"ì‹œë®¬ë ˆì´ì…˜ ê²€ìƒ‰ ê²°ê³¼ ìƒì„±: '{query}'")
        realistic_results: list[SearchResult] = []
        now_iso = datetime.now().isoformat()

        if "ë‚ ì”¨" in query or "ê¸°ìƒ" in query:
            realistic_results.extend([
                {
                    "title": "ì˜¤ëŠ˜ ì„œìš¸ ë‚ ì”¨: ë§‘ìŒ, ìµœê³  26ë„ - ê¸°ìƒì²­",
                    "url": "https://www.weather.go.kr/w/index.do",
                    "snippet": "ì„œìš¸ ì§€ì—­ ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ë§‘ê³  ìµœì € 18ë„, ìµœê³  26ë„ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. ë¯¸ì„¸ë¨¼ì§€ ë†ë„ëŠ” 'ë³´í†µ' ìˆ˜ì¤€ì´ë©°, ìì™¸ì„  ì§€ìˆ˜ëŠ” 'ë†’ìŒ'ì…ë‹ˆë‹¤.",
                    "publishedDate": now_iso,
                    "favicon": "https://www.google.com/s2/favicons?domain=weather.go.kr",
                },
                {
                    "title": "ì£¼ê°„ ë‚ ì”¨ ì „ë§: ë‚´ì¼ë¶€í„° ë¹„ ì†Œì‹ - ë„¤ì´ë²„ ë‚ ì”¨",
                    "url": "https://weather.naver.com/",
                    "snippet": "ë‚´ì¼ë¶€í„° ì „êµ­ì ìœ¼ë¡œ ë¹„ê°€ ë‚´ë¦´ ì „ë§ì…ë‹ˆë‹¤. ê°•ìˆ˜ëŸ‰ì€ 10~30mmë¡œ ì˜ˆìƒë˜ë©°, ìš°ì‚°ì„ ì±™ê¸°ì‹œê¸° ë°”ëë‹ˆë‹¤.",
                    "publishedDate": now_iso,
                    "favicon": "https://www.google.com/s2/favicons?domain=naver.com",
                },
            ])
        elif "ë‰´ìŠ¤" in query or "ì†Œì‹" in query:
            realistic_results.extend([
                {
                    "title": "ì˜¤ëŠ˜ì˜ ì£¼ìš” ë‰´ìŠ¤ - ì—°í•©ë‰´ìŠ¤",
                    "url": "https://www.yna.co.kr/",
                    "snippet": "ì •ë¶€, ì²­ë…„ ì£¼ê±° ì§€ì› ì •ì±… ë°œí‘œ... ì „êµ­ 5ë§Œ ê°€êµ¬ ê³µê¸‰ ê³„íš. ì•¼ë‹¹ 'ì‹¤íš¨ì„± ì˜ë¬¸' ë¹„íŒ.",
                    "publishedDate": now_iso,
                    "favicon": "https://www.google.com/s2/favicons?domain=yna.co.kr",
                },
                {
                    "title": "êµ­ì œ ì •ì„¸ ìµœì‹  ë™í–¥ - ì¤‘ì•™ì¼ë³´",
                    "url": "https://www.joongang.co.kr/",
                    "snippet": "ë¯¸-ì¤‘ ì •ìƒíšŒë‹´ ì´ë‹¬ ë§ ê°œìµœ ì˜ˆì •... ë¬´ì—­ ë¶„ìŸê³¼ ì•ˆë³´ ì´ìŠˆ ë…¼ì˜ ì „ë§.",
                    "publishedDate": now_iso,
                    "favicon": "https://www.google.com/s2/favicons?domain=joongang.co.kr",
                },
            ])
        
        # ê¸°ë³¸ ê²°ê³¼ ì¶”ê°€
        for i in range(num_results):
            if len(realistic_results) >= num_results:
                break
            realistic_results.append({
                "title": f"{query}ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ #{i+1}",
                "url": f"https://example.com/search?q={query.replace(' ', '+')}&page={i+1}",
                "snippet": f"'{query}'ì— ëŒ€í•œ ì‹œë®¬ë ˆì´ì…˜ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ê²ƒì€ ì‹¤ì œ ë°ì´í„°ê°€ ì•„ë‹Œ í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…ë‹ˆë‹¤. ê²°ê³¼ ë²ˆí˜¸: {i+1}",
                "publishedDate": now_iso,
                "favicon": "https://www.google.com/s2/favicons?domain=example.com",
            })

        return realistic_results[:num_results]

    async def _summarize_with_ai(self, query: str, results: list[SearchResult]) -> str:
        """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½"""
        if not self.client:
            return "AI ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        if not results:
            return "ìš”ì•½í•  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        logger.info(f"'{query}'ì— ëŒ€í•œ AI ìš”ì•½ ì‹œì‘...")
        
        try:
            snippets = "\n\n".join([
                f'Title: {res["title"]}\nSnippet: {res["snippet"]}'
                for res in results
            ])
            
            prompt = (
                f"ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ '{query}'ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” 3-4ë¬¸ì¥ì˜ ìš”ì•½ë¬¸ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì¤˜.\n\n"
                f"--- ê²€ìƒ‰ ê²°ê³¼ ---\n{snippets}\n\n--- ìš”ì•½ ---\n"
            )

            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=500,
            )
            
            summary = response.choices[0].message.content or "ìš”ì•½ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            logger.info("âœ… AI ìš”ì•½ ìƒì„± ì™„ë£Œ")
            return summary.strip()

        except Exception as e:
            logger.error(f"âŒ AI ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return "AI ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    async def search(
        self,
        query: str,
        source: str = "web",
        num_results: int = 5,
        include_summary: bool = True,
    ) -> tuple[str, list[SearchResult]]:
        """
        ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ , ì„ íƒì ìœ¼ë¡œ AI ìš”ì•½ì„ í¬í•¨í•©ë‹ˆë‹¤.

        Args:
            query: ê²€ìƒ‰ì–´
            source: ê²€ìƒ‰ ì†ŒìŠ¤ (web, research, etc.)
            num_results: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            include_summary: AI ìš”ì•½ í¬í•¨ ì—¬ë¶€

        Returns:
            (ìš”ì•½, ê²°ê³¼ ë¦¬ìŠ¤íŠ¸) íŠœí”Œ
        """
        start_time = time.monotonic()
        
        cache_key = self._generate_cache_key(query, source, num_results)
        
        # 1. ìºì‹œ í™•ì¸
        if self.cache_enabled and include_summary:
            cached = await self._get_cached_result(cache_key)
            if cached:
                self._update_stats(time.monotonic() - start_time)
                return cached["summary"], cached["results"]

        # 2. ìºì‹œ ì—†ìœ¼ë©´ MCP ê²€ìƒ‰ ìˆ˜í–‰
        results = await self._call_mcp_search(source, query, num_results)

        # 3. AI ìš”ì•½ ìƒì„± (í•„ìš” ì‹œ)
        summary = "ìš”ì•½ì´ ìš”ì²­ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        if include_summary:
            summary = await self._summarize_with_ai(query, results)

        # 4. ê²°ê³¼ ìºì‹œì— ì €ì¥ (ìš”ì•½ í¬í•¨ ì‹œ)
        if self.cache_enabled and include_summary:
            await self._set_cached_result(cache_key, summary, results)
        
        self._update_stats(time.monotonic() - start_time)
        return summary, results

    def _update_stats(self, response_time: float) -> None:
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        total = self.stats["total_searches"]
        current_avg = self.stats["avg_response_time"]
        new_avg_time = (current_avg * total + response_time) / (total + 1)
        
        self.stats["total_searches"] += 1
        self.stats["last_search_time"] = datetime.now().isoformat()
        self.stats["avg_response_time"] = new_avg_time

    def get_statistics(self) -> HandlerStats:
        """í•¸ë“¤ëŸ¬ ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        return self.stats

    async def clear_cache(self) -> bool:
        """ì›¹ ê²€ìƒ‰ê³¼ ê´€ë ¨ëœ ëª¨ë“  ìºì‹œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
        redis: AsyncRedis | None = await self._get_redis_client()
        if redis is None:
            logger.warning("ìºì‹œë¥¼ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: Redis í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© ë¶ˆê°€")
            return False

        try:
            # AsyncRedisë¡œ ìºìŠ¤íŒ…ëœ ê°ì²´ë¡œ keys í˜¸ì¶œ í›„ ë¦¬ìŠ¤íŠ¸ë¡œ ìºìŠ¤íŒ…
            keys: list[str] = await redis.keys("websearch:*")
            if not keys:
                logger.info("ì‚­ì œí•  ì›¹ ê²€ìƒ‰ ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return True

            await redis.delete(*keys)
            logger.info(f"{len(keys)}ê°œì˜ ì›¹ ê²€ìƒ‰ ìºì‹œë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
            return True
        except Exception as e:
            logger.error(f"ìºì‹œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False

    async def close(self) -> None:
        """Redis ì—°ê²°ì„ ì˜¬ë°”ë¥´ê²Œ ë‹«ìŠµë‹ˆë‹¤."""
        if self.redis_client:
            try:
                await self.redis_client.close()
                logger.info("Redis ì—°ê²°ì´ ì„±ê³µì ìœ¼ë¡œ ë‹«í˜”ìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.error(f"Redis ì—°ê²°ì„ ë‹«ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            finally:
                self.redis_client = None