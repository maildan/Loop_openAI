import streamlit as st
import requests
import json
import subprocess
from datetime import datetime
from typing import Generator, Any, TypedDict, cast, Literal


# --- íƒ€ì… ì •ì˜ ---
class OllamaModelInfo(TypedDict):
    name: str
    size: str
    quality: str
    speed: str


class OllamaListModel(TypedDict):
    name: str
    # ê¸°íƒ€ ollama listê°€ ë°˜í™˜í•˜ëŠ” í•„ë“œë“¤...


class StreamlitSessionState(TypedDict):
    """Streamlit ì„¸ì…˜ ìƒíƒœ íƒ€ì…"""
    messages: list[dict[Literal["role", "content"], str]]
    last_response: str


# ë¡œì»¬ ëª¨ë¸ ì„¤ì •
AVAILABLE_MODELS: dict[str, OllamaModelInfo] = {
    "solar:10.7b-instruct-v1-q4_K_M": {
        "name": "SOLAR 10.7B (í•œêµ­ì–´ íŠ¹í™”)",
        "size": "6.4GB",
        "quality": "â­â­â­â­â­",
        "speed": "ë¹ ë¦„",
    },
    "qwen2.5:7b-instruct-q4_K_M": {
        "name": "Qwen2.5 7B (ë‹¤êµ­ì–´)",
        "size": "4.4GB",
        "quality": "â­â­â­â­",
        "speed": "ë§¤ìš° ë¹ ë¦„",
    },
    "gemma2:2b-instruct-q4_K_M": {
        "name": "Gemma2 2B (ìµœê²½ëŸ‰)",
        "size": "1.7GB",
        "quality": "â­â­â­",
        "speed": "ì´ˆê³ ì†",
    },
}


def check_ollama_installed() -> bool:
    """Ollama ì„¤ì¹˜ í™•ì¸"""
    try:
        result = subprocess.run(
            ["ollama", "list"], capture_output=True, text=True, check=False
        )
        return result.returncode == 0
    except FileNotFoundError:
        return False


def install_ollama_guide() -> None:
    """Ollama ì„¤ì¹˜ ê°€ì´ë“œ"""
    _ = st.error("ğŸš¨ Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

    _ = st.markdown(
        """
    ### ğŸ“¥ Ollama ì„¤ì¹˜ ë°©ë²• (5ë¶„)
    
    #### macOS:
    ```bash
    brew install ollama
    ```
    
    #### Linux:
    ```bash
    curl -fsSL https://ollama.com/install.sh | sh
    ```
    
    #### Windows:
    1. https://ollama.com/download ì—ì„œ ë‹¤ìš´ë¡œë“œ
    2. ì„¤ì¹˜ í›„ í„°ë¯¸ë„ ì¬ì‹œì‘
    
    ### ğŸ”„ ì„¤ì¹˜ í›„ í•  ì¼:
    ```bash
    # 1. Ollama ì„œë¹„ìŠ¤ ì‹œì‘
    ollama serve
    
    # 2. í•œêµ­ì–´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìƒˆ í„°ë¯¸ë„ì—ì„œ)
    ollama pull solar:10.7b-instruct-v1-q4_K_M
    ```
    """
    )

    if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨ (ì„¤ì¹˜ í›„ í´ë¦­)"):
        st.rerun()


def get_available_models() -> list[str]:
    """ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        # JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ ìš”ì²­
        result = subprocess.run(
            ["ollama", "list", "--format", "json"],
            capture_output=True,
            text=True,
            check=False,
        )
        if result.returncode == 0:
            models_data = [
                json.loads(line) for line in result.stdout.strip().split("\n")
            ]
            typed_models = cast(list[OllamaListModel], models_data)
            return [model["name"] for model in typed_models]
        return []
    except (json.JSONDecodeError, FileNotFoundError):
        return []


def download_model(model_name: str) -> bool:
    """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
    try:
        process = subprocess.Popen(
            ["ollama", "pull", model_name],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding="utf-8",
        )

        # ì‹¤ì‹œê°„ ì§„í–‰ìƒí™© í‘œì‹œ
        progress_placeholder = st.empty()

        while True:
            if process.stdout is not None:
                output = process.stdout.readline()
                if output == "" and process.poll() is not None:
                    break
                if output:
                    _ = progress_placeholder.text(f"ë‹¤ìš´ë¡œë“œ ì¤‘: {output.strip()}")
            else:
                break

        return process.wait() == 0
    except FileNotFoundError:
        st.error("Ollama ëª…ë ¹ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„¤ì¹˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False
    except Exception as e:
        st.error(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False


def generate_with_ollama(
    prompt: str, model: str, temperature: float
) -> Generator[str, None, None]:
    """Ollamaë¡œ í•œêµ­ì–´ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)"""
    try:
        korean_prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ì°½ì‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ìš”ì²­: {prompt}

ìì—°ìŠ¤ëŸ½ê³  ì°½ì˜ì ì¸ í•œêµ­ì–´ë¡œ ì°½ì‘í•´ì£¼ì„¸ìš”. í•œêµ­ì  ì •ì„œì™€ ë¬¸í™”ë¥¼ ë°˜ì˜í•˜ì—¬ í¥ë¯¸ë¡œìš´ ì´ì•¼ê¸°ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”."""

        response_stream = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model,
                "prompt": korean_prompt,
                "stream": True,
                "options": {
                    "temperature": temperature,
                    "top_p": 0.9,
                    "num_predict": 1024,
                },
            },
            stream=True,
            timeout=120,
        )
        response_stream.raise_for_status()

        for line in response_stream.iter_lines():
            if line:
                try:
                    chunk = json.loads(line)
                    yield chunk.get("response", "")
                    if chunk.get("done"):
                        break
                except json.JSONDecodeError:
                    continue  # ê°€ë” ë¹ˆ ì¤„ì´ë‚˜ ë¶ˆì™„ì „í•œ JSONì´ ì˜¬ ìˆ˜ ìˆìŒ

    except requests.exceptions.ConnectionError:
        yield "âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'ollama serve' ëª…ë ¹ìœ¼ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”."
    except requests.exceptions.RequestException as e:
        yield f"API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
    except Exception as e:
        yield f"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def main() -> None:
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ í•¨ìˆ˜"""
    st.set_page_config(
        page_title="ğŸ†“ ì™„ì „ ë¬´ë£Œ í•œêµ­ì–´ AI", page_icon="ğŸ’°", layout="wide"
    )

    # --- ìƒíƒœ ì´ˆê¸°í™” ---
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–¤ ì´ì•¼ê¸°ë¥¼ ë§Œë“¤ì–´ë³¼ê¹Œìš”?"}
        ]
    if "last_response" not in st.session_state:
        st.session_state.last_response = ""
    
    # --- í—¤ë” ---
    _ = st.title("ğŸ†“ ì™„ì „ ë¬´ë£Œ í•œêµ­ì–´ ì°½ì‘ AI")
    _ = st.markdown("**OpenAI API ì—†ì´ë„ ì‘ë™í•˜ëŠ” ë¡œì»¬ AI** ğŸ’ª")

    # --- Ollama ì„¤ì¹˜ í™•ì¸ ---
    if not check_ollama_installed():
        install_ollama_guide()
        return

    # ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸
    installed_models = get_available_models()

    # --- ì‚¬ì´ë“œë°” ---
    with st.sidebar:
        _ = st.header("âš™ï¸ ì„¤ì •")

        # ëª¨ë¸ ì„ íƒ
        if installed_models:
            selected_model = st.selectbox(
                "ëª¨ë¸ ì„ íƒ",
                installed_models,
                help="ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”.",
                index=0
                if "solar:10.7b-instruct-v1-q4_K_M" not in installed_models
                else installed_models.index("solar:10.7b-instruct-v1-q4_K_M"),
            )
        else:
            _ = st.warning("ì„¤ì¹˜ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤! ë¨¼ì € ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.")
            selected_model = None

        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì„¹ì…˜
        _ = st.subheader("ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")

        for model_id, info in AVAILABLE_MODELS.items():
            with st.expander(f"ğŸ“¦ {info['name']}"):
                col1, col2 = st.columns([2, 1])
                with col1:
                    _ = st.write(f"**í¬ê¸°**: {info['size']}")
                    _ = st.write(f"**í’ˆì§ˆ**: {info['quality']}")
                    _ = st.write(f"**ì†ë„**: {info['speed']}")

                with col2:
                    if model_id in installed_models:
                        _ = st.success("âœ… ì„¤ì¹˜ë¨")
                    else:
                        if st.button("ë‹¤ìš´ë¡œë“œ", key=f"download_{model_id}"):
                            with st.spinner(f"{info['name']} ë‹¤ìš´ë¡œë“œ ì¤‘..."):
                                if download_model(model_id):
                                    st.success("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
                                    st.rerun()
                                else:
                                    st.error("ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")

        # ê³ ê¸‰ ì„¤ì •
        _ = st.subheader("ğŸ›ï¸ ìƒì„± ì„¤ì •")
        temperature = st.slider("ì°½ì˜ì„± (Temperature)", 0.1, 1.0, 0.8, 0.05)

        _ = st.info(
            "**ì°½ì˜ì„±**: ê°’ì´ ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•˜ê³  ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ì´ì•¼ê¸°ë¥¼, ë‚®ì„ìˆ˜ë¡ ì•ˆì •ì ì´ê³  ì¼ê´€ëœ ì´ì•¼ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
        )

        # ë¹„ìš© ë¹„êµ
        _ = st.subheader("ğŸ’° ë¹„ìš© ë¹„êµ")
        _ = st.write("**ë¡œì»¬ AI**: $0 (ì „ê¸°ë£Œë§Œ)")
        _ = st.write("**OpenAI API**: ì›” $5-50")
        _ = st.write("**ì ˆì•½ì•¡**: ì›” $60-600")

    # --- ë©”ì¸ ì½˜í…ì¸  ---
    if not selected_model:
        _ = st.warning("ì‚¬ì´ë“œë°”ì—ì„œ ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ê±°ë‚˜ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”!")
        return

    # ì´ì „ ëŒ€í™” ë‚´ìš© í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ì—¬ê¸°ì— ì°½ì‘ ìš”ì²­ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° í‘œì‹œ
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # AI ì‘ë‹µ ìƒì„± ë° í‘œì‹œ (ìŠ¤íŠ¸ë¦¬ë°)
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""
            for chunk in generate_with_ollama(prompt, selected_model, temperature):
                full_response += chunk
                response_placeholder.markdown(full_response + "â–Œ")
            response_placeholder.markdown(full_response)
        
        # AI ë©”ì‹œì§€ ì €ì¥
        st.session_state.messages.append({"role": "assistant", "content": full_response})
        st.session_state.last_response = full_response


if __name__ == "__main__":
    main()
