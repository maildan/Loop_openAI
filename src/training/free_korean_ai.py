import streamlit as st
import requests
import json
import subprocess
import os
from datetime import datetime

# ë¡œì»¬ ëª¨ë¸ ì„¤ì •
AVAILABLE_MODELS = {
    "solar:10.7b-instruct-v1-q4_K_M": {
        "name": "SOLAR 10.7B (í•œêµ­ì–´ íŠ¹í™”)",
        "size": "6.4GB",
        "quality": "â­â­â­â­â­",
        "speed": "ë¹ ë¦„"
    },
    "qwen2.5:7b-instruct-q4_K_M": {
        "name": "Qwen2.5 7B (ë‹¤êµ­ì–´)",
        "size": "4.4GB", 
        "quality": "â­â­â­â­",
        "speed": "ë§¤ìš° ë¹ ë¦„"
    },
    "llama3.2:3b-instruct-q4_K_M": {
        "name": "Llama 3.2 3B (ê²½ëŸ‰)",
        "size": "1.9GB",
        "quality": "â­â­â­",
        "speed": "ì´ˆê³ ì†"
    }
}

def check_ollama_installed():
    """Ollama ì„¤ì¹˜ í™•ì¸"""
    try:
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
        return result.returncode == 0
    except FileNotFoundError:
        return False

def install_ollama_guide():
    """Ollama ì„¤ì¹˜ ê°€ì´ë“œ"""
    st.error("ğŸš¨ Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
    
    st.markdown("""
    ### ğŸ“¥ Ollama ì„¤ì¹˜ ë°©ë²• (5ë¶„)
    
    #### macOS:
    ```bash
    brew install ollama
    ```
    
    #### Linux:
    ```bash
    curl -fsSL https://ollama.com/install.sh | sh
    ```
    
    #### Windows:
    1. https://ollama.com/download ì—ì„œ ë‹¤ìš´ë¡œë“œ
    2. ì„¤ì¹˜ í›„ í„°ë¯¸ë„ ì¬ì‹œì‘
    
    ### ğŸ”„ ì„¤ì¹˜ í›„ í•  ì¼:
    ```bash
    # 1. Ollama ì„œë¹„ìŠ¤ ì‹œì‘
    ollama serve
    
    # 2. í•œêµ­ì–´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìƒˆ í„°ë¯¸ë„ì—ì„œ)
    ollama pull solar:10.7b-instruct-v1-q4_K_M
    ```
    """)
    
    if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨ (ì„¤ì¹˜ í›„ í´ë¦­)"):
        st.rerun()

def get_available_models():
    """ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')[1:]  # í—¤ë” ì œì™¸
            models = []
            for line in lines:
                if line.strip():
                    model_name = line.split()[0]
                    models.append(model_name)
            return models
        return []
    except:
        return []

def download_model(model_name):
    """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
    try:
        process = subprocess.Popen(
            ['ollama', 'pull', model_name],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        # ì‹¤ì‹œê°„ ì§„í–‰ìƒí™© í‘œì‹œ
        progress_placeholder = st.empty()
        
        while True:
            if process.stdout is not None:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    progress_placeholder.text(f"ë‹¤ìš´ë¡œë“œ ì¤‘: {output.strip()}")
            else:
                break
        
        return process.returncode == 0
    except:
        return False

def generate_with_ollama(prompt, model="solar:10.7b-instruct-v1-q4_K_M", temperature=0.8):
    """Ollamaë¡œ í•œêµ­ì–´ ìƒì„±"""
    try:
        korean_prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ì°½ì‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ìš”ì²­: {prompt}

ìì—°ìŠ¤ëŸ½ê³  ì°½ì˜ì ì¸ í•œêµ­ì–´ë¡œ ì°½ì‘í•´ì£¼ì„¸ìš”. í•œêµ­ì  ì •ì„œì™€ ë¬¸í™”ë¥¼ ë°˜ì˜í•˜ì—¬ í¥ë¯¸ë¡œìš´ ì´ì•¼ê¸°ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”."""

        response = requests.post('http://localhost:11434/api/generate',
            json={
                'model': model,
                'prompt': korean_prompt,
                'stream': False,
                'options': {
                    'temperature': temperature,
                    'top_p': 0.9,
                    'max_tokens': 1000
                }
            },
            timeout=60
        )
        
        if response.status_code == 200:
            return json.loads(response.text)['response']
        else:
            return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: HTTP {response.status_code}"
            
    except requests.exceptions.ConnectionError:
        return "âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'ollama serve' ëª…ë ¹ìœ¼ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”."
    except Exception as e:
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def main():
    st.set_page_config(
        page_title="ğŸ†“ ì™„ì „ ë¬´ë£Œ í•œêµ­ì–´ AI",
        page_icon="ğŸ’°",
        layout="wide"
    )
    
    # í—¤ë”
    st.title("ğŸ†“ ì™„ì „ ë¬´ë£Œ í•œêµ­ì–´ ì°½ì‘ AI")
    st.markdown("**OpenAI API ì—†ì´ë„ ì‘ë™í•˜ëŠ” ë¡œì»¬ AI** ğŸ’ª")
    
    # Ollama ì„¤ì¹˜ í™•ì¸
    if not check_ollama_installed():
        install_ollama_guide()
        return
    
    # ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸
    installed_models = get_available_models()
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # ëª¨ë¸ ì„ íƒ
        if installed_models:
            selected_model = st.selectbox(
                "ëª¨ë¸ ì„ íƒ",
                installed_models,
                help="ì„¤ì¹˜ëœ ëª¨ë¸ ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”"
            )
        else:
            st.warning("ì„¤ì¹˜ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
            selected_model = None
        
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì„¹ì…˜
        st.subheader("ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
        
        for model_id, info in AVAILABLE_MODELS.items():
            with st.expander(f"ğŸ“¦ {info['name']}"):
                col1, col2 = st.columns([2, 1])
                with col1:
                    st.write(f"**í¬ê¸°**: {info['size']}")
                    st.write(f"**í’ˆì§ˆ**: {info['quality']}")
                    st.write(f"**ì†ë„**: {info['speed']}")
                
                with col2:
                    if model_id in installed_models:
                        st.success("âœ… ì„¤ì¹˜ë¨")
                    else:
                        if st.button(f"ë‹¤ìš´ë¡œë“œ", key=f"download_{model_id}"):
                            with st.spinner(f"{info['name']} ë‹¤ìš´ë¡œë“œ ì¤‘..."):
                                if download_model(model_id):
                                    st.success("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
                                    st.rerun()
                                else:
                                    st.error("ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        
        # ê³ ê¸‰ ì„¤ì •
        st.subheader("ğŸ›ï¸ ìƒì„± ì„¤ì •")
        temperature = st.slider("ì°½ì˜ì„±", 0.1, 1.0, 0.8, 0.1)
        
        # ë¹„ìš© ë¹„êµ
        st.subheader("ğŸ’° ë¹„ìš© ë¹„êµ")
        st.write("**ë¡œì»¬ AI**: $0 (ì „ê¸°ë£Œë§Œ)")
        st.write("**OpenAI API**: ì›” $5-50")
        st.write("**ì ˆì•½ì•¡**: ì›” $60-600")
    
    # ë©”ì¸ ì½˜í…ì¸ 
    if not installed_models:
        st.warning("ë¨¼ì € ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”!")
        return
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("ğŸ“ ì°½ì‘ ìš”ì²­")
        
        # ì¥ë¥´ë³„ ì˜ˆì‹œ
        genre_examples = {
            "íŒíƒ€ì§€": "ë§ˆë²•ì‚¬ í•™êµì— ì…í•™í•œ í‰ë²”í•œ í•™ìƒì˜ ëª¨í—˜",
            "ë¡œë§¨ìŠ¤": "ì¹´í˜ì—ì„œ ë§Œë‚œ ë‘ ì‚¬ëŒì˜ ìš´ëª…ì  ë§Œë‚¨",
            "SF": "AIê°€ ê°ì •ì„ ê°€ì§€ê²Œ ë˜ë©´ì„œ ë²Œì–´ì§€ëŠ” ì¼",
            "ë¯¸ìŠ¤í„°ë¦¬": "ì‚¬ë¼ì§„ ì¹œêµ¬ë¥¼ ì°¾ëŠ” ëŒ€í•™ìƒì˜ ì¶”ë¦¬",
            "ì¼ìƒ": "í‰ë²”í•œ íšŒì‚¬ì›ì˜ íŠ¹ë³„í•œ í•˜ë£¨"
        }
        
        # ì¥ë¥´ ì„ íƒ
        selected_genre = st.selectbox("ì¥ë¥´ ì„ íƒ", list(genre_examples.keys()))
        example_text = genre_examples[selected_genre]
        
        # í”„ë¡¬í”„íŠ¸ ì…ë ¥
        prompt = st.text_area(
            "ì°½ì‘ ìš”ì²­ì„ ì…ë ¥í•˜ì„¸ìš”",
            height=150,
            placeholder=f"ì˜ˆì‹œ: {example_text}"
        )
        
        # ìƒì„± ë²„íŠ¼
        if st.button("âœ¨ ë¬´ë£Œë¡œ ì°½ì‘í•˜ê¸°", type="primary"):
            if prompt.strip() and selected_model is not None:
                with st.spinner("ì°½ì‘ ì¤‘... (ë¡œì»¬ì—ì„œ ì²˜ë¦¬ ì¤‘) â³"):
                    result = generate_with_ollama(prompt, selected_model, temperature)
                    st.session_state.result = result
                    st.session_state.prompt = prompt
                    st.session_state.model = selected_model
                    st.session_state.timestamp = datetime.now()
            else:
                if not prompt.strip():
                    st.warning("ì°½ì‘ ìš”ì²­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
                else:
                    st.warning("ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”!")
    
    with col2:
        st.subheader("ğŸ“– ì°½ì‘ ê²°ê³¼")
        
        if 'result' in st.session_state:
            # ê²°ê³¼ í‘œì‹œ
            st.markdown("### ìƒì„±ëœ ì‘í’ˆ")
            st.write(st.session_state.result)
            
            # ë©”íƒ€ ì •ë³´
            st.markdown("---")
            col_info1, col_info2, col_info3 = st.columns(3)
            with col_info1:
                st.write(f"**ëª¨ë¸**: {st.session_state.model.split(':')[0]}")
            with col_info2:
                st.write(f"**ìƒì„± ì‹œê°„**: {st.session_state.timestamp.strftime('%H:%M:%S')}")
            with col_info3:
                word_count = len(st.session_state.result.replace(' ', ''))
                st.write(f"**ê¸€ì ìˆ˜**: {word_count:,}ì")
            
            # ì•¡ì…˜ ë²„íŠ¼ë“¤
            st.markdown("---")
            col_btn1, col_btn2, col_btn3 = st.columns(3)
            
            with col_btn1:
                st.download_button(
                    label="ğŸ’¾ ë‹¤ìš´ë¡œë“œ",
                    data=st.session_state.result,
                    file_name=f"ì°½ì‘ë¬¼_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                    mime="text/plain"
                )
            
            with col_btn2:
                if st.button("â• ì´ì–´ì“°ê¸°"):
                    if selected_model is not None:
                        continue_prompt = f"ë‹¤ìŒ ë‚´ìš©ì„ ì´ì–´ì„œ ì¨ì£¼ì„¸ìš”:\n\n{st.session_state.result}\n\nê³„ì†:"
                        with st.spinner("ì´ì–´ì“°ê¸° ì¤‘..."):
                            continued = generate_with_ollama(continue_prompt, selected_model, temperature)
                            st.session_state.result += "\n\n" + continued
                            st.rerun()
                    else:
                        st.warning("ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”!")
            
            with col_btn3:
                if st.button("ğŸ”„ ë‹¤ì‹œ ìƒì„±"):
                    if selected_model is not None:
                        with st.spinner("ë‹¤ì‹œ ìƒì„± ì¤‘..."):
                            new_result = generate_with_ollama(st.session_state.prompt, selected_model, temperature)
                            st.session_state.result = new_result
                            st.rerun()
                    else:
                        st.warning("ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”!")
        
        else:
            st.info("ì™¼ìª½ì—ì„œ ì°½ì‘ ìš”ì²­ì„ ì…ë ¥í•˜ê³  'ë¬´ë£Œë¡œ ì°½ì‘í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!")
    
    # ì„±ëŠ¥ ë¹„êµ
    st.markdown("---")
    st.subheader("ğŸ“Š ì„±ëŠ¥ ë¹„êµ")
    
    comparison_data = {
        "íŠ¹ì§•": ["ë¹„ìš©", "ì†ë„", "í”„ë¼ì´ë²„ì‹œ", "ì˜¤í”„ë¼ì¸", "í’ˆì§ˆ"],
        "ë¡œì»¬ AI (ë¬´ë£Œ)": ["$0", "ë¹ ë¦„", "100% ì•ˆì „", "ê°€ëŠ¥", "â­â­â­â­"],
        "OpenAI API": ["ì›” $5-50", "ë§¤ìš° ë¹ ë¦„", "ë°ì´í„° ì „ì†¡", "ë¶ˆê°€ëŠ¥", "â­â­â­â­â­"]
    }
    
    st.table(comparison_data)
    
    # í‘¸í„°
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #666;'>
        <p>ğŸ†“ <strong>ì™„ì „ ë¬´ë£Œ í•œêµ­ì–´ AI</strong> ğŸ†“</p>
        <p>OpenAI API ì—†ì´ë„ ì¶©ë¶„íˆ ì¢‹ì€ í’ˆì§ˆ!</p>
        <p>ğŸ’° <strong>ì›” $60-600 ì ˆì•½</strong> ğŸ’°</p>
        <p><em>"ë¬´ë£Œê°€ ìµœê³ ë‹¤!" - ëª¨ë“  ê°œë°œì</em></p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main() 