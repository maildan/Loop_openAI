import streamlit as st
import requests
import json
import subprocess
import os
import time
from datetime import datetime

# OpenAI ê°€ì ¸ì˜¤ê¸° (ì„ íƒì )
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

class HybridKoreanAI:
    def __init__(self):
        self.openai_key = os.getenv("OPEN_API_KEY") or os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY", "")
        self.use_openai = bool(self.openai_key) and OPENAI_AVAILABLE
        
        if self.use_openai:
            openai.api_key = self.openai_key
    
    def check_ollama_available(self):
        """Ollama ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            response = requests.get('http://localhost:11434/api/tags', timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
        models = {}
        
        # OpenAI ëª¨ë¸
        if self.use_openai:
            models.update({
                "gpt-4o-mini": {
                    "name": "GPT-4o Mini (OpenAI)",
                    "type": "api",
                    "cost": "ì›” $5-10",
                    "quality": "â­â­â­â­â­",
                    "speed": "ë§¤ìš° ë¹ ë¦„"
                },
                "gpt-3.5-turbo": {
                    "name": "GPT-3.5 Turbo (OpenAI)",
                    "type": "api", 
                    "cost": "ì›” $3-8",
                    "quality": "â­â­â­â­",
                    "speed": "ì´ˆê³ ì†"
                }
            })
        
        # ë¡œì»¬ ëª¨ë¸ (Ollama)
        if self.check_ollama_available():
            try:
                response = requests.get('http://localhost:11434/api/tags')
                if response.status_code == 200:
                    ollama_models = response.json().get('models', [])
                    for model in ollama_models:
                        model_name = model['name']
                        models[f"local_{model_name}"] = {
                            "name": f"{model_name} (ë¡œì»¬)",
                            "type": "local",
                            "cost": "ë¬´ë£Œ",
                            "quality": "â­â­â­â­" if "solar" in model_name else "â­â­â­",
                            "speed": "ë¹ ë¦„"
                        }
            except:
                pass
        
        return models
    
    def generate_with_openai(self, prompt, model="gpt-4o-mini", temperature=0.8, progress_callback=None):
        """OpenAI APIë¡œ ìƒì„± (ì§„í–‰ìƒí™© í‘œì‹œ í¬í•¨)"""
        try:
            if progress_callback:
                progress_callback(10, "ğŸ¤– OpenAI API ì¤€ë¹„ ì¤‘...")
            
            system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ì°½ì‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

íŠ¹ì§•:
- ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ì‚¬ìš©
- ì°½ì˜ì ì´ê³  í¥ë¯¸ë¡œìš´ ìŠ¤í† ë¦¬
- ì ì ˆí•œ ë†’ì„ë²•ê³¼ ë¬¸ì²´
- í•œêµ­ì  ì •ì„œì™€ ë¬¸í™” ë°˜ì˜

ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ ì†Œì„¤, ì‹œë‚˜ë¦¬ì˜¤, ì—ì„¸ì´ ë“±ì„ ì°½ì‘í•´ì£¼ì„¸ìš”."""

            if progress_callback:
                progress_callback(30, "âœï¸ ì°½ì‘ ì‹œì‘...")

            response = openai.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=temperature,
                top_p=0.9,
                stream=True  # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ íƒ€ì„ì•„ì›ƒ ë°©ì§€
            )
            
            if progress_callback:
                progress_callback(60, "ğŸ“ í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
            full_response = ""
            for chunk in response:
                if chunk.choices[0].delta.content is not None:
                    full_response += chunk.choices[0].delta.content
            
            if progress_callback:
                progress_callback(100, "ğŸ‰ ì™„ë£Œ!")
            
            return full_response
        except Exception as e:
            return f"OpenAI API ì˜¤ë¥˜: {str(e)}"
    
    def generate_with_ollama(self, prompt, model, temperature=0.8):
        """Ollamaë¡œ ìƒì„±"""
        try:
            # local_ ì ‘ë‘ì‚¬ ì œê±°
            actual_model = model.replace("local_", "")
            
            korean_prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ì°½ì‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ìš”ì²­: {prompt}

ìì—°ìŠ¤ëŸ½ê³  ì°½ì˜ì ì¸ í•œêµ­ì–´ë¡œ ì°½ì‘í•´ì£¼ì„¸ìš”. í•œêµ­ì  ì •ì„œì™€ ë¬¸í™”ë¥¼ ë°˜ì˜í•˜ì—¬ í¥ë¯¸ë¡œìš´ ì´ì•¼ê¸°ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”."""

            response = requests.post('http://localhost:11434/api/generate',
                json={
                    'model': actual_model,
                    'prompt': korean_prompt,
                    'stream': False,
                    'options': {
                        'temperature': temperature,
                        'top_p': 0.9,
                        'max_tokens': 1000
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                return json.loads(response.text)['response']
            else:
                return f"Ollama ì˜¤ë¥˜: HTTP {response.status_code}"
                
        except Exception as e:
            return f"ë¡œì»¬ ëª¨ë¸ ì˜¤ë¥˜: {str(e)}"
    
    def generate(self, prompt, model_id, temperature=0.8):
        """í†µí•© ìƒì„± í•¨ìˆ˜"""
        models = self.get_available_models()
        
        if model_id not in models:
            return "ì„ íƒëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        model_info = models[model_id]
        
        if model_info['type'] == 'api':
            return self.generate_with_openai(prompt, model_id, temperature)
        elif model_info['type'] == 'local':
            return self.generate_with_ollama(prompt, model_id, temperature)
        else:
            return "ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…ì…ë‹ˆë‹¤."

def setup_guide():
    """ì„¤ì • ê°€ì´ë“œ"""
    st.markdown("""
    ## ğŸ”§ ì„¤ì • ê°€ì´ë“œ
    
    ### Option 1: OpenAI API ì‚¬ìš© (ê³ í’ˆì§ˆ)
    1. OpenAI API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •:
    ```bash
    export OPENAI_API_KEY="sk-your-key-here"
    ```
    2. ë˜ëŠ” Streamlit secretsì— ì¶”ê°€
    3. **ë¹„ìš©**: ì›” $5-10 (ë§¤ìš° ì €ë ´)
    4. **í’ˆì§ˆ**: â­â­â­â­â­
    
    ### Option 2: ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© (ë¬´ë£Œ)
    1. Ollama ì„¤ì¹˜:
    ```bash
    # macOS
    brew install ollama
    
    # Linux
    curl -fsSL https://ollama.com/install.sh | sh
    ```
    2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ:
    ```bash
    ollama pull solar:10.7b-instruct-v1-q4_K_M
    ```
    3. ì„œë²„ ì‹œì‘:
    ```bash
    ollama serve
    ```
    4. **ë¹„ìš©**: ë¬´ë£Œ (ì „ê¸°ë£Œë§Œ)
    5. **í’ˆì§ˆ**: â­â­â­â­
    
    ### ğŸ¯ ê¶Œì¥ì‚¬í•­
    - **ë¹ ë¥¸ ì‹œì‘**: OpenAI API (30ë¶„ ì„¤ì •)
    - **ì¥ê¸° ì‚¬ìš©**: ë¡œì»¬ ëª¨ë¸ (ë¬´ë£Œ)
    - **ìµœê³  í’ˆì§ˆ**: í•˜ì´ë¸Œë¦¬ë“œ (ë‘˜ ë‹¤ ì‚¬ìš©)
    """)

def main():
    st.set_page_config(
        page_title="ğŸ”€ í•˜ì´ë¸Œë¦¬ë“œ í•œêµ­ì–´ AI",
        page_icon="âš¡",
        layout="wide"
    )
    
    # í—¤ë”
    st.title("ğŸ”€ í•˜ì´ë¸Œë¦¬ë“œ í•œêµ­ì–´ ì°½ì‘ AI")
    st.markdown("**OpenAI API + ë¡œì»¬ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ì„ íƒí•˜ëŠ” ìŠ¤ë§ˆíŠ¸ AI** âš¡")
    
    # AI ì´ˆê¸°í™”
    ai = HybridKoreanAI()
    available_models = ai.get_available_models()
    
    # ì„¤ì • ìƒíƒœ í‘œì‹œ
    col_status1, col_status2, col_status3 = st.columns(3)
    
    with col_status1:
        if ai.use_openai:
            st.success("âœ… OpenAI API ì—°ê²°ë¨")
        else:
            st.warning("âš ï¸ OpenAI API ì—†ìŒ")
    
    with col_status2:
        if ai.check_ollama_available():
            st.success("âœ… ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
        else:
            st.warning("âš ï¸ ë¡œì»¬ ëª¨ë¸ ì—†ìŒ")
    
    with col_status3:
        total_models = len(available_models)
        if total_models > 0:
            st.info(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {total_models}ê°œ")
        else:
            st.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì—†ìŒ")
    
    # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì„¤ì • ê°€ì´ë“œ í‘œì‹œ
    if not available_models:
        setup_guide()
        return
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # ëª¨ë¸ ì„ íƒ
        st.subheader("ğŸ¤– ëª¨ë¸ ì„ íƒ")
        
        # ëª¨ë¸ì„ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
        api_models = {k: v for k, v in available_models.items() if v['type'] == 'api'}
        local_models = {k: v for k, v in available_models.items() if v['type'] == 'local'}
        
        model_options = []
        if api_models:
            model_options.append("--- OpenAI API ëª¨ë¸ ---")
            model_options.extend(list(api_models.keys()))
        
        if local_models:
            model_options.append("--- ë¡œì»¬ ëª¨ë¸ ---")
            model_options.extend(list(local_models.keys()))
        
        # êµ¬ë¶„ì„  ì œê±°í•œ ì‹¤ì œ ëª¨ë¸ë§Œ í•„í„°ë§
        actual_models = [m for m in model_options if not m.startswith("---")]
        
        selected_model = st.selectbox(
            "ëª¨ë¸ ì„ íƒ",
            actual_models,
            format_func=lambda x: available_models[x]['name'] if x in available_models else x
        )
        
        # ì„ íƒëœ ëª¨ë¸ ì •ë³´ í‘œì‹œ
        if selected_model in available_models:
            model_info = available_models[selected_model]
            st.info(f"""
            **ì„ íƒëœ ëª¨ë¸**: {model_info['name']}
            **ë¹„ìš©**: {model_info['cost']}
            **í’ˆì§ˆ**: {model_info['quality']}
            **ì†ë„**: {model_info['speed']}
            """)
        
        # ìƒì„± ì„¤ì •
        st.subheader("ğŸ›ï¸ ìƒì„± ì„¤ì •")
        temperature = st.slider("ì°½ì˜ì„±", 0.1, 1.0, 0.8, 0.1)
        
        # ë¹„ìš© ë¹„êµ
        st.subheader("ğŸ’° ë¹„ìš© ë¹„êµ")
        if ai.use_openai:
            st.success("OpenAI API: ì›” $5-10")
        else:
            st.info("OpenAI API: ì„¤ì • ì•ˆë¨")
        
        if ai.check_ollama_available():
            st.success("ë¡œì»¬ ëª¨ë¸: ë¬´ë£Œ")
        else:
            st.info("ë¡œì»¬ ëª¨ë¸: ì„¤ì • ì•ˆë¨")
        
        # ì¶”ì²œ ì„¤ì •
        st.subheader("ğŸ’¡ ì¶”ì²œ ì„¤ì •")
        if st.button("ğŸš€ ë¹ ë¥¸ ì‹œì‘ (API)"):
            if api_models:
                selected_model = list(api_models.keys())[0]
                st.rerun()
        
        if st.button("ğŸ’° ë¬´ë£Œ ì‚¬ìš© (ë¡œì»¬)"):
            if local_models:
                selected_model = list(local_models.keys())[0]
                st.rerun()
    
    # ë©”ì¸ ì½˜í…ì¸ 
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("ğŸ“ ì°½ì‘ ìš”ì²­")
        
        # ì¥ë¥´ë³„ ì˜ˆì‹œ
        genre_examples = {
            "íŒíƒ€ì§€ ì†Œì„¤": "ë§ˆë²• í•™êµì— ì…í•™í•œ í‰ë²”í•œ í•™ìƒì´ ìˆ¨ê²¨ì§„ ëŠ¥ë ¥ì„ ë°œê²¬í•˜ëŠ” ì´ì•¼ê¸°ë¥¼ ì¨ì£¼ì„¸ìš”.",
            "ë¡œë§¨ìŠ¤ ì†Œì„¤": "ì¹´í˜ì—ì„œ ìš°ì—°íˆ ë§Œë‚œ ë‘ ì‚¬ëŒì´ ìš´ëª…ì ì¸ ì‚¬ë‘ì— ë¹ ì§€ëŠ” ì´ì•¼ê¸°ë¥¼ ì¨ì£¼ì„¸ìš”.",
            "SF ì†Œì„¤": "AIê°€ ì¸ê°„ì˜ ê°ì •ì„ ì´í•´í•˜ê²Œ ë˜ë©´ì„œ ë²Œì–´ì§€ëŠ” ì¼ì„ ê·¸ë¦° ì´ì•¼ê¸°ë¥¼ ì¨ì£¼ì„¸ìš”.",
            "ë¯¸ìŠ¤í„°ë¦¬": "ëŒ€í•™êµì—ì„œ ì¼ì–´ë‚œ ì—°ì‡„ ì‹¤ì¢… ì‚¬ê±´ì„ ìˆ˜ì‚¬í•˜ëŠ” í˜•ì‚¬ì˜ ì´ì•¼ê¸°ë¥¼ ì¨ì£¼ì„¸ìš”.",
            "ì¼ìƒ ë“œë¼ë§ˆ": "í‰ë²”í•œ íšŒì‚¬ì›ì´ ê²ªëŠ” íŠ¹ë³„í•œ í•˜ë£¨ë¥¼ ê·¸ë¦° ê°ë™ì ì¸ ì´ì•¼ê¸°ë¥¼ ì¨ì£¼ì„¸ìš”."
        }
        
        # ì¥ë¥´ ì„ íƒ
        selected_genre = st.selectbox("ì¥ë¥´ ì„ íƒ", list(genre_examples.keys()))
        example_text = genre_examples[selected_genre]
        
        # í”„ë¡¬í”„íŠ¸ ì…ë ¥
        prompt = st.text_area(
            "ì°½ì‘ ìš”ì²­ì„ ì…ë ¥í•˜ì„¸ìš”",
            height=150,
            placeholder=example_text
        )
        
        # ì˜ˆì‹œ ë²„íŠ¼ë“¤
        st.markdown("**ë¹ ë¥¸ ì˜ˆì‹œ:**")
        col_ex1, col_ex2 = st.columns(2)
        
        with col_ex1:
            if st.button("ğŸ“– ì˜ˆì‹œ ì‚¬ìš©"):
                prompt = example_text
                st.rerun()
        
        with col_ex2:
            if st.button("ğŸ² ëœë¤ ì˜ˆì‹œ"):
                import random
                random_genre = random.choice(list(genre_examples.keys()))
                prompt = genre_examples[random_genre]
                st.rerun()
        
        # ìƒì„± ë²„íŠ¼
        if st.button("âœ¨ ì°½ì‘í•˜ê¸°", type="primary"):
            if prompt.strip() and selected_model:
                model_info = available_models[selected_model]
                cost_info = f"({model_info['cost']})" if model_info['cost'] != "ë¬´ë£Œ" else "(ë¬´ë£Œ)"
                
                with st.spinner(f"ì°½ì‘ ì¤‘... {cost_info} â³"):
                    raw_result = ai.generate(prompt, selected_model, temperature)
                    result = str(raw_result) if raw_result is not None else ""
                    st.session_state.result = result
                    st.session_state.prompt = prompt
                    st.session_state.model = selected_model
                    st.session_state.model_info = model_info
                    st.session_state.timestamp = datetime.now()
            else:
                st.warning("ì°½ì‘ ìš”ì²­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
    
    with col2:
        st.subheader("ğŸ“– ì°½ì‘ ê²°ê³¼")
        
        if 'result' in st.session_state:
            # ê²°ê³¼ í‘œì‹œ
            st.markdown("### ìƒì„±ëœ ì‘í’ˆ")
            st.write(st.session_state.result)
            
            # ë©”íƒ€ ì •ë³´
            st.markdown("---")
            col_info1, col_info2, col_info3 = st.columns(3)
            with col_info1:
                st.write(f"**ëª¨ë¸**: {st.session_state.model_info['name']}")
            with col_info2:
                st.write(f"**ë¹„ìš©**: {st.session_state.model_info['cost']}")
            with col_info3:
                word_count = len(str(st.session_state.result or "").replace(' ', ''))
                st.write(f"**ê¸€ì ìˆ˜**: {word_count:,}ì")
            
            # í’ˆì§ˆ í‰ê°€
            st.markdown("---")
            st.write("**ì´ ê²°ê³¼ê°€ ì–´ë– ì…¨ë‚˜ìš”?**")
            col_rating1, col_rating2, col_rating3, col_rating4 = st.columns(4)
            
            with col_rating1:
                if st.button("ğŸ˜ í›Œë¥­í•¨"):
                    st.success("í”¼ë“œë°± ê°ì‚¬í•©ë‹ˆë‹¤!")
            with col_rating2:
                if st.button("ğŸ‘ ì¢‹ìŒ"):
                    st.success("í”¼ë“œë°± ê°ì‚¬í•©ë‹ˆë‹¤!")
            with col_rating3:
                if st.button("ğŸ‘Œ ë³´í†µ"):
                    st.info("ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ìœ„í•´ ë…¸ë ¥í•˜ê² ìŠµë‹ˆë‹¤!")
            with col_rating4:
                if st.button("ğŸ‘ ì•„ì‰¬ì›€"):
                    st.info("ë‹¤ë¥¸ ëª¨ë¸ì´ë‚˜ ì„¤ì •ì„ ì‹œë„í•´ë³´ì„¸ìš”!")
            
            # ì•¡ì…˜ ë²„íŠ¼ë“¤
            st.markdown("---")
            col_btn1, col_btn2, col_btn3 = st.columns(3)
            
            with col_btn1:
                st.download_button(
                    label="ğŸ’¾ ë‹¤ìš´ë¡œë“œ",
                    data=str(st.session_state.result or ""),
                    file_name=f"ì°½ì‘ë¬¼_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                    mime="text/plain"
                )
            
            with col_btn2:
                if st.button("â• ì´ì–´ì“°ê¸°"):
                    if st.session_state.model is not None:
                        continue_prompt = f"ë‹¤ìŒ ë‚´ìš©ì„ ì´ì–´ì„œ ì¨ì£¼ì„¸ìš”:\n\n{st.session_state.result}\n\nê³„ì†:"
                        with st.spinner("ì´ì–´ì“°ê¸° ì¤‘..."):
                            cont_raw = ai.generate(continue_prompt, st.session_state.model, temperature)
                            continued = str(cont_raw) if cont_raw is not None else ""
                            st.session_state.result = (st.session_state.result or "") + "\n\n" + continued
                            st.rerun()
                    else:
                        st.warning("ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”!")
            
            with col_btn3:
                if st.button("ğŸ”„ ë‹¤ì‹œ ìƒì„±"):
                    with st.spinner("ë‹¤ì‹œ ìƒì„± ì¤‘..."):
                        new_raw = ai.generate(st.session_state.prompt, st.session_state.model, temperature)
                        new_result = str(new_raw) if new_raw is not None else ""
                        st.session_state.result = new_result
                        st.rerun()
        
        else:
            st.info("ì™¼ìª½ì—ì„œ ì°½ì‘ ìš”ì²­ì„ ì…ë ¥í•˜ê³  'ì°½ì‘í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!")
    
    # ëª¨ë¸ ë¹„êµ ì„¹ì…˜
    if len(available_models) > 1:
        st.markdown("---")
        st.subheader("ğŸ“Š ëª¨ë¸ ë¹„êµ")
        
        comparison_data = []
        for model_id, info in available_models.items():
            comparison_data.append({
                "ëª¨ë¸": info['name'],
                "íƒ€ì…": "API" if info['type'] == 'api' else "ë¡œì»¬",
                "ë¹„ìš©": info['cost'],
                "í’ˆì§ˆ": info['quality'],
                "ì†ë„": info['speed']
            })
        
        st.table(comparison_data)
    
    # íŒ ì„¹ì…˜
    st.markdown("---")
    st.subheader("ğŸ’¡ ì‚¬ìš© íŒ")
    
    col_tip1, col_tip2, col_tip3 = st.columns(3)
    
    with col_tip1:
        st.markdown("""
        **ğŸ’° ë¹„ìš© ì ˆì•½**
        - ë¡œì»¬ ëª¨ë¸ë¡œ ì´ˆì•ˆ ì‘ì„±
        - API ëª¨ë¸ë¡œ ìµœì¢… ë‹¤ë“¬ê¸°
        - ê°„ë‹¨í•œ ìš”ì²­ì€ ë¡œì»¬ ì‚¬ìš©
        """)
    
    with col_tip2:
        st.markdown("""
        **âš¡ ì†ë„ ìµœì í™”**
        - ì§§ì€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        - êµ¬ì²´ì ì¸ ìš”ì²­í•˜ê¸°
        - ì ì ˆí•œ ì˜¨ë„ ì„¤ì •
        """)
    
    with col_tip3:
        st.markdown("""
        **ğŸ¯ í’ˆì§ˆ í–¥ìƒ**
        - êµ¬ì²´ì ì¸ ì¥ë¥´ ì§€ì •
        - ì›í•˜ëŠ” ë¶„ìœ„ê¸° ëª…ì‹œ
        - ì˜ˆì‹œ í¬í•¨í•´ì„œ ìš”ì²­
        """)
    
    # í‘¸í„°
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #666;'>
        <p>ğŸ”€ <strong>í•˜ì´ë¸Œë¦¬ë“œ í•œêµ­ì–´ AI</strong> ğŸ”€</p>
        <p>ìµœê³ ì˜ í’ˆì§ˆê³¼ ìµœì ì˜ ë¹„ìš©ì„ ë™ì‹œì—!</p>
        <p><em>"ì„ íƒì˜ ììœ ê°€ ì§„ì§œ ììœ ë‹¤!" - ìŠ¤ë§ˆíŠ¸í•œ ê°œë°œì</em></p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main() 