import os
from typing import Any, Callable, cast, TypedDict

import requests
import streamlit as st

# OpenAI ê°€ì ¸ì˜¤ê¸° (ì„ íƒì )
try:
    import openai
    # OpenAIì˜ CompletionCreateê³¼ ê°™ì€ íƒ€ì…ì„ ì§ì ‘ ì‚¬ìš©í•˜ê¸° ìœ„í•¨
    from openai.types.chat import ChatCompletionMessageParam

    openai_available = True
except ImportError:
    openai_available = False
    openai = None
    ChatCompletionMessageParam = None  # type: ignore


class ModelInfo(TypedDict):
    name: str
    type: str
    cost: str


class ChatMessage(TypedDict):
    role: str
    content: str

# --- Ollama API ì‘ë‹µì„ ìœ„í•œ TypedDict ---
class OllamaModel(TypedDict):
    name: str
    model: str
    modified_at: str
    size: int
    digest: str
    details: dict[str, Any]

class OllamaTagsResponse(TypedDict):
    models: list[OllamaModel]

class OllamaChatResponseMessage(TypedDict):
    role: str
    content: str

class OllamaChatResponse(TypedDict):
    model: str
    created_at: str
    message: OllamaChatResponseMessage
    done: bool


class HybridKoreanAI:
    def __init__(self) -> None:
        self.openai_key: str | None = os.getenv("OPEN_API_KEY") or os.getenv(
            "OPENAI_API_KEY"
        )
        if not self.openai_key:
            try:
                self.openai_key = st.secrets.get("OPENAI_API_KEY", "")
            except Exception:
                self.openai_key = ""

        self.use_openai: bool = bool(self.openai_key) and openai_available

        if self.use_openai and openai:
            openai.api_key = self.openai_key

    def get_available_models(self) -> dict[str, ModelInfo]:
        models: dict[str, ModelInfo] = {}
        if self.use_openai:
            models.update(
                {
                    "gpt-4o-mini": {
                        "name": "GPT-4o Mini (OpenAI)",
                        "type": "api",
                        "cost": "$0.15/M input, $0.60/M output",
                    },
                    "gpt-4-turbo": {
                        "name": "GPT-4 Turbo (OpenAI)",
                        "type": "api",
                        "cost": "$10/M input, $30/M output",
                    },
                }
            )

        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=2)
            if response.status_code == 200:
                local_models_response: OllamaTagsResponse = response.json()
                local_models = local_models_response.get("models", [])
                for model in local_models:
                    model_name = model.get("name", "unknown")
                    models[f"local_{model_name}"] = {
                        "name": f"{model_name} (ë¡œì»¬)",
                        "type": "local",
                        "cost": "ë¬´ë£Œ",
                    }
        except requests.RequestException:
            pass  # ë¡œì»¬ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ì¡°ìš©íˆ ì‹¤íŒ¨
        return models

    def generate_response(
        self,
        model: str,
        messages: list[ChatMessage],
        temperature: float,
        progress_callback: Callable[[int, str], None],
    ) -> str:
        model_info = self.get_available_models().get(model)
        if not model_info:
            return "ì˜¤ë¥˜: ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        try:
            if model_info["type"] == "api":
                if not (self.use_openai and openai and ChatCompletionMessageParam):
                    return "ì˜¤ë¥˜: OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ê±°ë‚˜ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                progress_callback(10, "OpenAI API í˜¸ì¶œ ì¤‘...")

                # messages íƒ€ì…ì„ ChatCompletionMessageParam ë¦¬ìŠ¤íŠ¸ë¡œ ìºìŠ¤íŒ…
                typed_messages = cast(list[ChatCompletionMessageParam], messages)

                response = openai.chat.completions.create(
                    model=model,
                    messages=typed_messages,
                    temperature=temperature,
                    stream=True,
                )

                full_response = ""
                for chunk in response:
                    content = chunk.choices[0].delta.content
                    if content:
                        full_response += content
                progress_callback(100, "ì™„ë£Œ")
                return full_response

            elif model_info["type"] == "local":
                progress_callback(10, "ë¡œì»¬ ëª¨ë¸ í˜¸ì¶œ ì¤‘...")
                actual_model_name = model.replace("local_", "")
                response = requests.post(
                    "http://localhost:11434/api/chat",
                    json={
                        "model": actual_model_name,
                        "messages": messages,
                        "stream": False,
                        "options": {"temperature": temperature},
                    },
                    timeout=120,
                )
                response.raise_for_status()
                progress_callback(100, "ì™„ë£Œ")
                response_data: OllamaChatResponse = response.json()
                return response_data.get("message", {"content": ""}).get("content", "")

        except requests.RequestException as e:
            return f"ì˜¤ë¥˜: ë¡œì»¬ ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨ - {e}"
        except Exception as e:
            if openai and isinstance(e, openai.APIError):
                return f"ì˜¤ë¥˜: OpenAI API ë¬¸ì œ ë°œìƒ - {e}"
            return f"ì˜¤ë¥˜: ìƒì„± ì¤‘ ë¬¸ì œ ë°œìƒ - {e}"
        return "ì˜¤ë¥˜: ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ íƒ€ì…"


def initialize_session_state() -> None:
    """Streamlit ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if "messages" not in st.session_state:
        st.session_state.messages = []


def main_page(ai: HybridKoreanAI, available_models: dict[str, ModelInfo]) -> None:
    _ = st.set_page_config(page_title="í•˜ì´ë¸Œë¦¬ë“œ AI", layout="centered")
    _ = st.title("ğŸ”€ í•˜ì´ë¸Œë¦¬ë“œ í•œêµ­ì–´ AI")

    initialize_session_state()

    # --- ì‚¬ì´ë“œë°” ---
    with st.sidebar:
        _ = st.header("âš™ï¸ ì„¤ì •")
        model_options = list(available_models.keys())

        if not model_options:
            _ = st.warning("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. Ollamaë¥¼ ì‹¤í–‰í•˜ê±°ë‚˜ OpenAI í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
            st.stop()

        selected_model = st.selectbox(
            "ëª¨ë¸ ì„ íƒ",
            options=model_options,
            format_func=lambda x: str(available_models.get(x, {"name": x}).get("name", x)),
        )

        temperature = st.slider("ì°½ì˜ì„± (Temperature)", 0.0, 1.0, 0.7, 0.05)

        _ = st.header("ëŒ€í™” ê¸°ë¡")
        if st.button("ìƒˆ ëŒ€í™” ì‹œì‘"):
            st.session_state.messages = []
            st.rerun()

    # --- ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---
    messages = cast(list[ChatMessage], st.session_state.messages)
    for message in messages:
        with st.chat_message(message["role"]):
            _ = st.markdown(message["content"])

    if prompt := st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"):
        messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            _ = st.markdown(prompt)

        with st.chat_message("assistant"):
            progress_bar = st.progress(0, "AIê°€ ìƒê° ì¤‘...")

            def progress_callback(progress: int, text: str) -> None:
                progress_bar.progress(progress, text)

            if selected_model:
                response_content = ai.generate_response(
                    model=selected_model,
                    messages=messages,
                    temperature=temperature,
                    progress_callback=progress_callback,
                )
                _ = st.markdown(response_content)
                messages.append({"role": "assistant", "content": response_content})
            else:
                _ = st.warning("ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    ai_instance = HybridKoreanAI()
    models = ai_instance.get_available_models()
    main_page(ai_instance, models) 