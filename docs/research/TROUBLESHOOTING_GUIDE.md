# Loop AI Troubleshooting Guide: ì‹¤ì „ ë¬¸ì œ í•´ê²° ë§¤ë‰´ì–¼

## ğŸš¨ Emergency Response Guide

**ì´ ë¬¸ì„œëŠ” Loop AI í”„ë¡œì íŠ¸ì—ì„œ ì‹¤ì œë¡œ ë°œìƒí•œ ëª¨ë“  ë¬¸ì œì™€ í•´ê²° ê³¼ì •ì„ ê¸°ë¡í•œ ì‹¤ì „ ë§¤ë‰´ì–¼ì…ë‹ˆë‹¤.**

## ğŸ“‹ Quick Fix Index

| ë¬¸ì œ ìœ í˜• | ì‹¬ê°ë„ | í•´ê²° ì‹œê°„ | í˜ì´ì§€ |
|-----------|--------|-----------|--------|
| Catastrophic Forgetting | ğŸ”´ Critical | 4ì‹œê°„ | [#catastrophic-forgetting](#1-catastrophic-forgetting) |
| Import ì—ëŸ¬ | ğŸŸ¡ Medium | 30ë¶„ | [#import-errors](#2-import-errors) |
| MPS ë©”ëª¨ë¦¬ ë¶€ì¡± | ğŸ”´ Critical | 2ì‹œê°„ | [#mps-memory-issues](#3-mps-memory-issues) |
| í† í°í™” ë¬¸ì œ | ğŸŸ  High | 1ì‹œê°„ | [#tokenization-issues](#4-tokenization-issues) |
| íƒ€ì… íŒíŠ¸ ì—ëŸ¬ | ğŸŸ¡ Medium | 15ë¶„ | [#type-hint-errors](#5-type-hint-errors) |
| ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ | ğŸ”´ Critical | 3ì‹œê°„ | [#model-loading-failures](#6-model-loading-failures) |

## ğŸ”¥ Critical Issues (ì¦‰ì‹œ í•´ê²° í•„ìš”)

### 1. Catastrophic Forgetting

#### ğŸš¨ ì¦ìƒ
```
ë¬¸ì œ: íŒŒì¸íŠœë‹ í›„ ëª¨ë¸ì´ ì™„ì „íˆ ë§ê°€ì§
- í•œêµ­ì–´ â†’ ì¤‘êµ­ì–´/ì˜ì–´ í˜¼ì¬ ì¶œë ¥
- HTML íƒœê·¸, ì½”ë“œ ìƒì„±
- ì˜ë¯¸ ì—†ëŠ” í…ìŠ¤íŠ¸ ìƒì„±
```

#### ğŸ” ì›ì¸ ë¶„ì„
```python
# ë¬¸ì œê°€ ëœ íŒŒì¸íŠœë‹ ì„¤ì •
training_args = TrainingArguments(
    per_device_train_batch_size=8,    # ë„ˆë¬´ í° ë°°ì¹˜ í¬ê¸°
    learning_rate=5e-4,               # ë„ˆë¬´ ë†’ì€ í•™ìŠµë¥ 
    num_train_epochs=10,              # ê³¼ë„í•œ ì—í¬í¬
    gradient_checkpointing=True,      # MPS í˜¸í™˜ì„± ë¬¸ì œ
    dataloader_num_workers=4,         # MPSì—ì„œ ë¬¸ì œ ë°œìƒ
)

# ë¬¸ì œê°€ ëœ ë°ì´í„° êµ¬ì¡°
{
    "text": ["ëŒ€í™”1", "ëŒ€í™”2", "ëŒ€í™”3"],  # JSON êµ¬ì¡°ì— ê³¼ì í•©
    "metadata": {"genre": "fantasy"}
}
```

#### âœ… í•´ê²° ë°©ë²•

**ì‘ê¸‰ì²˜ì¹˜ (ì¦‰ì‹œ ì ìš©)**
```python
# 1. ì›ë³¸ ëª¨ë¸ë¡œ ë³µê·€
base_model_name = "Qwen/Qwen2.5-0.5B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    torch_dtype=torch.float16,
    trust_remote_code=True
)

# 2. ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
def create_emergency_prompt(user_request: str) -> str:
    return f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ì°½ì‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì˜ˆì‹œ:
ì‚¬ìš©ì: íŒíƒ€ì§€ ì†Œì„¤ì„ ì¨ì£¼ì„¸ìš”
AI: ì—˜ë¼ë¼ëŠ” ë§ˆë²• ì•„ì¹´ë°ë¯¸ ìµœí•˜ìœ„ í•™ìƒì´ì—ˆë‹¤. ë‹¤ë¥¸ í•™ìƒë“¤ì´ í™”ë ¤í•œ ë§ˆë²•ì„ ì„ ë³´ì¼ ë•Œ, ê·¸ë…€ëŠ” ê²¨ìš° ì‘ì€ ë¶ˆê½ƒ í•˜ë‚˜ ë§Œë“¤ì–´ë‚´ëŠ” ê²ƒì´ ê³ ì‘ì´ì—ˆë‹¤.

ì‚¬ìš©ì: {user_request}
í•œêµ­ì–´ ì°½ì‘ ì‹œì‘:"""

# 3. í•œêµ­ì–´ í•„í„°ë§ ì ìš©
def emergency_korean_filter(text: str) -> str:
    # ì˜ì–´/ì¤‘êµ­ì–´/HTML ì œê±°
    korean_only = re.sub(r'[a-zA-Z]{3,}', '', text)
    korean_only = re.sub(r'[\u4e00-\u9fff]+', '', text)
    korean_only = re.sub(r'<[^>]+>', '', text)
    return korean_only.strip()
```

**ê·¼ë³¸ì  í•´ê²°ì±…**
```python
# 1. ì•ˆì „í•œ íŒŒì¸íŠœë‹ ì„¤ì •
safe_training_args = TrainingArguments(
    per_device_train_batch_size=2,        # ì‘ì€ ë°°ì¹˜
    learning_rate=1e-5,                   # ë‚®ì€ í•™ìŠµë¥ 
    num_train_epochs=3,                   # ì ì€ ì—í¬í¬
    gradient_checkpointing=False,         # MPSì—ì„œ ë¹„í™œì„±í™”
    dataloader_num_workers=0,             # MPS í˜¸í™˜ì„±
    warmup_steps=100,                     # ì ì§„ì  í•™ìŠµ
    save_steps=50,                        # ìì£¼ ì €ì¥
    eval_steps=50,                        # ìì£¼ í‰ê°€
    logging_steps=10,                     # ìƒì„¸ ë¡œê¹…
)

# 2. ë°ì´í„° êµ¬ì¡° ê°œì„ 
def convert_to_natural_format(json_data):
    """JSON êµ¬ì¡°ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    if isinstance(json_data.get('text'), list):
        # ëŒ€í™” ë¦¬ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¡œ ë³€í™˜
        conversation = ""
        for i, utterance in enumerate(json_data['text']):
            speaker = "A" if i % 2 == 0 else "B"
            conversation += f"{speaker}: {utterance}\n"
        return conversation
    return json_data.get('text', '')
```

#### ğŸ“Š íš¨ê³¼ ì¸¡ì •
```python
# ë³µêµ¬ ì „í›„ ë¹„êµ
before_fix = {
    'korean_purity': 0.23,      # 23% í•œêµ­ì–´
    'coherence': 0.15,          # 15% ì¼ê´€ì„±
    'usability': 0.08           # 8% ì‚¬ìš©ì„±
}

after_emergency_fix = {
    'korean_purity': 0.78,      # 78% í•œêµ­ì–´ (+239%)
    'coherence': 0.65,          # 65% ì¼ê´€ì„± (+333%)
    'usability': 0.72           # 72% ì‚¬ìš©ì„± (+800%)
}
```

### 2. Import Errors

#### ğŸš¨ ì¦ìƒ
```bash
ImportError: cannot import name 'TrainingArguments' from 'transformers'
ImportError: cannot import name 'str' from 'typing'
ModuleNotFoundError: No module named 'peft'
```

#### ğŸ” ì›ì¸ ë¶„ì„
```python
# ë¬¸ì œê°€ ëœ import êµ¬ë¬¸ë“¤
from transformers import TrainingArguments, Trainer  # íŒ¨í‚¤ì§€ ëˆ„ë½
from typing import str                               # ì˜ëª»ëœ import
from peft import LoraConfig                         # íŒ¨í‚¤ì§€ ë¯¸ì„¤ì¹˜
```

#### âœ… í•´ê²° ë°©ë²•

**ì¦‰ì‹œ í•´ê²°**
```bash
# 1. ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸
source venv/bin/activate

# 2. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install transformers==4.36.0
pip install peft==0.7.1
pip install torch==2.1.0
pip install accelerate==0.25.0

# 3. ì˜ì¡´ì„± ì¶©ëŒ í•´ê²°
pip install --upgrade transformers peft torch
```

**ì •í™•í•œ import êµ¬ë¬¸**
```python
# âœ… ì˜¬ë°”ë¥¸ import
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from typing import Optional, List, Dict, Union  # str ì œì™¸
import torch
import torch.nn as nn
```

**ë²„ì „ í˜¸í™˜ì„± ë§¤íŠ¸ë¦­ìŠ¤**
```python
compatible_versions = {
    'transformers': '4.36.0',
    'peft': '0.7.1', 
    'torch': '2.1.0',
    'accelerate': '0.25.0',
    'python': '3.10.x'
}
```

### 3. MPS Memory Issues

#### ğŸš¨ ì¦ìƒ
```
RuntimeError: MPS backend out of memory (MPS allocated: 17.11 GB, other allocations: 1.02 GB, max allowed: 18.13 GB). Tried to allocate 1.16 GB on private pool.
```

#### ğŸ” ì›ì¸ ë¶„ì„
```python
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
memory_breakdown = {
    'base_model': '8.5 GB',         # Qwen2.5-0.5B FP16
    'optimizer_states': '4.2 GB',   # AdamW ìƒíƒœ
    'gradients': '2.1 GB',          # ê·¸ë˜ë””ì–¸íŠ¸ ì €ì¥
    'activation_cache': '3.3 GB',   # ìˆœì „íŒŒ ìºì‹œ
    'batch_data': '1.8 GB',         # ë°°ì¹˜ ë°ì´í„°
    'total': '19.9 GB'              # > 18.13 GB í•œê³„ ì´ˆê³¼
}
```

#### âœ… í•´ê²° ë°©ë²•

**ì‘ê¸‰ ë©”ëª¨ë¦¬ í•´ì œ**
```python
import torch
import gc

def emergency_memory_cleanup():
    """ì‘ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬"""
    
    # 1. ìºì‹œ ì •ë¦¬
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()
    
    # 2. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
    gc.collect()
    
    # 3. ëª¨ë¸ ì–¸ë¡œë“œ
    if 'model' in globals():
        del model
    
    # 4. í† í¬ë‚˜ì´ì € ì–¸ë¡œë“œ  
    if 'tokenizer' in globals():
        del tokenizer
    
    print("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

# ì‚¬ìš©ë²•
emergency_memory_cleanup()
```

**ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •**
```python
# 1. ëª¨ë¸ ìµœì í™”
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,      # FP16 ì‚¬ìš© (ë©”ëª¨ë¦¬ 50% ì ˆì•½)
    low_cpu_mem_usage=True,         # CPU ë©”ëª¨ë¦¬ ì ˆì•½
    device_map="auto"               # ìë™ ë””ë°”ì´ìŠ¤ í• ë‹¹
)

# 2. ë°°ì¹˜ í¬ê¸° ìµœì í™”
def find_optimal_batch_size():
    """ìµœì  ë°°ì¹˜ í¬ê¸° íƒìƒ‰"""
    for batch_size in [1, 2, 4, 8]:
        try:
            # í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ìƒì„±
            test_batch = create_test_batch(batch_size)
            
            # ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸
            with torch.no_grad():
                outputs = model(**test_batch)
            
            print(f"âœ… ë°°ì¹˜ í¬ê¸° {batch_size}: ì„±ê³µ")
            return batch_size
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"âŒ ë°°ì¹˜ í¬ê¸° {batch_size}: ë©”ëª¨ë¦¬ ë¶€ì¡±")
                continue
            else:
                raise e
    
    return 1  # ìµœì†Œ ë°°ì¹˜ í¬ê¸°

optimal_batch_size = find_optimal_batch_size()

# 3. ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… (MPS í˜¸í™˜)
training_args = TrainingArguments(
    per_device_train_batch_size=optimal_batch_size,
    gradient_checkpointing=False,    # MPSì—ì„œ ë¹„í™œì„±í™”
    dataloader_pin_memory=False,     # MPS í˜¸í™˜ì„±
    fp16=False,                      # MPSëŠ” FP16 ìë™ ì²˜ë¦¬
    bf16=False,                      # MPS ë¯¸ì§€ì›
)
```

**ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§**
```python
def monitor_memory_usage():
    """ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§"""
    
    if torch.backends.mps.is_available():
        allocated = torch.mps.current_allocated_memory() / 1024**3
        reserved = torch.mps.driver_allocated_memory() / 1024**3
        
        print(f"ğŸ MPS ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
        print(f"  í• ë‹¹ë¨: {allocated:.2f} GB")
        print(f"  ì˜ˆì•½ë¨: {reserved:.2f} GB")
        print(f"  ì‚¬ìš©ë¥ : {(allocated/18.13)*100:.1f}%")
        
        if allocated > 16.0:  # 16GB ì´ìƒ ì‚¬ìš©ì‹œ ê²½ê³ 
            print("âš ï¸  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤!")
            return False
    
    return True

# í•™ìŠµ ì¤‘ ì£¼ê¸°ì  ëª¨ë‹ˆí„°ë§
class MemoryCallback:
    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step % 10 == 0:
            if not monitor_memory_usage():
                control.should_training_stop = True
```

### 4. Tokenization Issues

#### ğŸš¨ ì¦ìƒ
```python
# ë¬¸ì œ: í•œêµ­ì–´ í† í°í™” ë¹„íš¨ìœ¨ì„±
text = "ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”."
tokens = tokenizer.tokenize(text)
# ê²°ê³¼: 18ê°œ í† í° (ë¹„íš¨ìœ¨ì )
```

#### ğŸ” ì›ì¸ ë¶„ì„
```python
# Qwen í† í¬ë‚˜ì´ì €ì˜ í•œêµ­ì–´ ì²˜ë¦¬ ë¬¸ì œ
tokenizer_analysis = {
    'vocab_size': 151936,
    'korean_tokens': 0.15,      # ì „ì²´ì˜ 15%ë§Œ í•œêµ­ì–´
    'efficiency': 'low',        # í•œêµ­ì–´ ì²˜ë¦¬ ë¹„íš¨ìœ¨ì 
    'subword_splitting': 'aggressive'  # ê³¼ë„í•œ ë¶„í• 
}
```

#### âœ… í•´ê²° ë°©ë²•

**í† í¬ë‚˜ì´ì € ìµœì í™”**
```python
def optimize_korean_tokenization(tokenizer):
    """í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ìµœì í™”"""
    
    # 1. í•œêµ­ì–´ íŠ¹ìˆ˜ í† í° ì¶”ê°€
    korean_special_tokens = [
        "[KOR]",      # í•œêµ­ì–´ ì‹œì‘
        "[/KOR]",     # í•œêµ­ì–´ ì¢…ë£Œ
        "[FORMAL]",   # ë†’ì„ë²•
        "[INFORMAL]", # ë°˜ë§
        "[CREATIVE]"  # ì°½ì‘ ëª¨ë“œ
    ]
    
    tokenizer.add_special_tokens({
        'additional_special_tokens': korean_special_tokens
    })
    
    # 2. í•œêµ­ì–´ ìš°ì„  í† í°í™”
    def korean_friendly_encode(text):
        # í•œêµ­ì–´ êµ¬ë¬¸ ë³´í˜¸
        text = f"[KOR]{text}[/KOR]"
        return tokenizer.encode(text)
    
    return korean_friendly_encode

# ì‚¬ìš©ë²•
optimized_encode = optimize_korean_tokenization(tokenizer)
```

**íš¨ìœ¨ì„± ë¹„êµ**
```python
def compare_tokenization_efficiency():
    """í† í¬ë‚˜ì´ì € íš¨ìœ¨ì„± ë¹„êµ"""
    
    test_sentences = [
        "ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.",
        "ì£„ì†¡í•©ë‹ˆë‹¤ë§Œ, ë„ì›€ì´ í•„ìš”í•©ë‹ˆë‹¤.",
        "ê·¸ ì˜í™” ì •ë§ ì¬ë¯¸ìˆì—ˆì–´ìš”!"
    ]
    
    results = {}
    
    for sentence in test_sentences:
        # ì›ë³¸ í† í¬ë‚˜ì´ì €
        original_tokens = tokenizer.tokenize(sentence)
        
        # ìµœì í™”ëœ í† í¬ë‚˜ì´ì €
        optimized_tokens = optimized_encode(sentence)
        
        results[sentence] = {
            'original': len(original_tokens),
            'optimized': len(optimized_tokens),
            'improvement': len(original_tokens) - len(optimized_tokens)
        }
    
    return results

efficiency_results = compare_tokenization_efficiency()
```

### 5. Type Hint Errors

#### ğŸš¨ ì¦ìƒ
```python
# ì—ëŸ¬ ë©”ì‹œì§€ë“¤
TypeError: 'type' object is not subscriptable
ImportError: cannot import name 'str' from 'typing'
AttributeError: 'Tensor' object has no attribute 'flatten'
```

#### ğŸ” ì›ì¸ ë¶„ì„
```python
# ë¬¸ì œê°€ ëœ íƒ€ì… íŒíŠ¸ë“¤
from typing import str                    # âŒ ì˜ëª»ëœ import
def process_data(data: list[str]):       # âŒ Python 3.8 í˜¸í™˜ì„±
def analyze(tensor: Tensor) -> bool:     # âŒ íƒ€ì… ë¯¸ì •ì˜
```

#### âœ… í•´ê²° ë°©ë²•

**ì˜¬ë°”ë¥¸ íƒ€ì… íŒíŠ¸**
```python
# âœ… ì •í™•í•œ íƒ€ì… íŒíŠ¸
from typing import List, Dict, Optional, Union, Any, Tuple
import torch
from torch import Tensor

# Python ë²„ì „ë³„ í˜¸í™˜ì„±
import sys
if sys.version_info >= (3, 9):
    from typing import List as ListType
else:
    from typing import List as ListType

# ì˜¬ë°”ë¥¸ í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜
def process_data(data: List[str]) -> Dict[str, Any]:
    """ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜"""
    return {}

def analyze_tensor(tensor: torch.Tensor) -> bool:
    """í…ì„œ ë¶„ì„ í•¨ìˆ˜"""
    return tensor.numel() > 0

def train_model(
    model: torch.nn.Module,
    data: List[Dict[str, Union[str, int]]],
    epochs: int = 10
) -> Optional[torch.nn.Module]:
    """ëª¨ë¸ í›ˆë ¨ í•¨ìˆ˜"""
    return model
```

**íƒ€ì… ì²´í¬ ë„êµ¬ ì„¤ì •**
```python
# mypy ì„¤ì • (pyproject.toml)
[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true

# ì‹¤í–‰ ì‹œ íƒ€ì… ì²´í¬
def runtime_type_check(func):
    """ëŸ°íƒ€ì„ íƒ€ì… ì²´í¬ ë°ì½”ë ˆì´í„°"""
    import inspect
    from typing import get_type_hints
    
    def wrapper(*args, **kwargs):
        sig = inspect.signature(func)
        hints = get_type_hints(func)
        
        # ë§¤ê°œë³€ìˆ˜ íƒ€ì… ì²´í¬
        for param_name, param_value in zip(sig.parameters.keys(), args):
            if param_name in hints:
                expected_type = hints[param_name]
                if not isinstance(param_value, expected_type):
                    raise TypeError(
                        f"{param_name}: expected {expected_type}, "
                        f"got {type(param_value)}"
                    )
        
        return func(*args, **kwargs)
    
    return wrapper

# ì‚¬ìš©ë²•
@runtime_type_check
def safe_function(text: str, count: int) -> str:
    return text * count
```

### 6. Model Loading Failures

#### ğŸš¨ ì¦ìƒ
```python
# ë‹¤ì–‘í•œ ëª¨ë¸ ë¡œë”© ì—ëŸ¬
OSError: Can't load tokenizer for 'Qwen/Qwen2.5-0.5B-Instruct'
RuntimeError: Error loading state_dict
AttributeError: 'Qwen2VLConfig' has no attribute 'vision_start_token_id'
```

#### ğŸ” ì›ì¸ ë¶„ì„
```python
# ë¬¸ì œ ì›ì¸ë“¤
loading_issues = {
    'wrong_model_class': 'Qwen2VLForConditionalGeneration ì‚¬ìš©',
    'missing_files': 'ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë¶ˆì™„ì „',
    'version_mismatch': 'transformers ë²„ì „ ë¶ˆì¼ì¹˜',
    'device_conflict': 'CPU/GPU/MPS ë””ë°”ì´ìŠ¤ ì¶©ëŒ'
}
```

#### âœ… í•´ê²° ë°©ë²•

**ì•ˆì „í•œ ëª¨ë¸ ë¡œë”©**
```python
def safe_model_loading(model_name: str, max_retries: int = 3):
    """ì•ˆì „í•œ ëª¨ë¸ ë¡œë”© í•¨ìˆ˜"""
    
    for attempt in range(max_retries):
        try:
            print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œë„ {attempt + 1}/{max_retries}")
            
            # 1. í† í¬ë‚˜ì´ì € ë¨¼ì € ë¡œë“œ
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                use_fast=False  # ì•ˆì •ì„± ìš°ì„ 
            )
            print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
            
            # 2. ëª¨ë¸ ë¡œë“œ
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            
            # 3. ë””ë°”ì´ìŠ¤ ì„¤ì •
            device = get_optimal_device()
            model.to(device)
            print(f"âœ… {device} ë””ë°”ì´ìŠ¤ ì„¤ì • ì™„ë£Œ")
            
            # 4. ê²€ì¦ í…ŒìŠ¤íŠ¸
            test_input = tokenizer("ì•ˆë…•í•˜ì„¸ìš”", return_tensors="pt")
            test_input = {k: v.to(device) for k, v in test_input.items()}
            
            with torch.no_grad():
                outputs = model(**test_input)
            
            print("âœ… ëª¨ë¸ ê²€ì¦ ì™„ë£Œ")
            return model, tokenizer
            
        except Exception as e:
            print(f"âŒ ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {str(e)}")
            
            if attempt < max_retries - 1:
                # ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ì¬ì‹œë„
                emergency_memory_cleanup()
                time.sleep(5)
            else:
                print("ğŸš¨ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨")
                raise e

def get_optimal_device():
    """ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ"""
    if torch.backends.mps.is_available():
        return torch.device("mps")
    elif torch.cuda.is_available():
        return torch.device("cuda")
    else:
        return torch.device("cpu")

# ì‚¬ìš©ë²•
try:
    model, tokenizer = safe_model_loading("Qwen/Qwen2.5-0.5B-Instruct")
except Exception as e:
    print(f"ëª¨ë¸ ë¡œë”© ìµœì¢… ì‹¤íŒ¨: {e}")
    # ëŒ€ì•ˆ ëª¨ë¸ ì‹œë„
    model, tokenizer = safe_model_loading("microsoft/DialoGPT-medium")
```

**ëª¨ë¸ ìƒíƒœ ì§„ë‹¨**
```python
def diagnose_model_state(model, tokenizer):
    """ëª¨ë¸ ìƒíƒœ ì§„ë‹¨"""
    
    diagnosis = {
        'model_type': type(model).__name__,
        'model_size': sum(p.numel() for p in model.parameters()),
        'trainable_params': sum(p.numel() for p in model.parameters() if p.requires_grad),
        'device': next(model.parameters()).device,
        'dtype': next(model.parameters()).dtype,
        'vocab_size': tokenizer.vocab_size,
        'special_tokens': len(tokenizer.special_tokens_map)
    }
    
    print("ğŸ” ëª¨ë¸ ì§„ë‹¨ ê²°ê³¼:")
    for key, value in diagnosis.items():
        print(f"  {key}: {value}")
    
    # ê±´ê°•ì„± ì²´í¬
    health_score = 0
    if diagnosis['trainable_params'] > 0:
        health_score += 25
    if 'mps' in str(diagnosis['device']) or 'cuda' in str(diagnosis['device']):
        health_score += 25
    if diagnosis['vocab_size'] > 50000:
        health_score += 25
    if diagnosis['model_size'] > 100000000:  # 100M+ parameters
        health_score += 25
    
    print(f"ğŸ¥ ëª¨ë¸ ê±´ê°•ë„: {health_score}/100")
    return health_score >= 75

# ì‚¬ìš©ë²•
is_healthy = diagnose_model_state(model, tokenizer)
if not is_healthy:
    print("âš ï¸ ëª¨ë¸ ìƒíƒœê°€ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤!")
```

## ğŸŸ¡ Medium Priority Issues

### 7. Training Instability

#### ğŸš¨ ì¦ìƒ
```
Lossê°€ ë°œì‚°í•˜ê±°ë‚˜ NaN ë°œìƒ
Gradient exploding/vanishing
í•™ìŠµì´ ì§„í–‰ë˜ì§€ ì•ŠìŒ
```

#### âœ… í•´ê²° ë°©ë²•

**ì•ˆì •ì ì¸ í•™ìŠµ ì„¤ì •**
```python
def create_stable_training_config():
    """ì•ˆì •ì ì¸ í•™ìŠµ ì„¤ì •"""
    
    return TrainingArguments(
        # ê¸°ë³¸ ì„¤ì •
        output_dir="./stable_training",
        num_train_epochs=3,
        
        # ë°°ì¹˜ ë° í•™ìŠµë¥ 
        per_device_train_batch_size=2,
        per_device_eval_batch_size=4,
        learning_rate=1e-5,           # ë‚®ì€ í•™ìŠµë¥ 
        warmup_steps=100,             # ì›Œë°ì—…
        
        # ì •ê·œí™”
        weight_decay=0.01,            # L2 ì •ê·œí™”
        max_grad_norm=1.0,            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        
        # í‰ê°€ ë° ì €ì¥
        eval_steps=50,
        save_steps=100,
        save_total_limit=3,
        
        # ë¡œê¹…
        logging_steps=10,
        logging_dir="./logs",
        
        # ì•ˆì •ì„±
        dataloader_num_workers=0,     # MPS í˜¸í™˜
        remove_unused_columns=False,
        
        # ì¡°ê¸° ì¢…ë£Œ
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
    )

# Loss ëª¨ë‹ˆí„°ë§
class LossMonitor:
    def __init__(self, patience=5):
        self.patience = patience
        self.best_loss = float('inf')
        self.patience_counter = 0
        self.loss_history = []
    
    def check_loss(self, current_loss):
        self.loss_history.append(current_loss)
        
        # NaN ì²´í¬
        if math.isnan(current_loss):
            print("ğŸš¨ Lossê°€ NaNì…ë‹ˆë‹¤! í•™ìŠµ ì¤‘ë‹¨.")
            return False
        
        # ë°œì‚° ì²´í¬
        if current_loss > self.best_loss * 2:
            self.patience_counter += 1
            print(f"âš ï¸ Loss ì¦ê°€ ê°ì§€ ({self.patience_counter}/{self.patience})")
            
            if self.patience_counter >= self.patience:
                print("ğŸ›‘ Loss ë°œì‚°ìœ¼ë¡œ í•™ìŠµ ì¤‘ë‹¨.")
                return False
        else:
            if current_loss < self.best_loss:
                self.best_loss = current_loss
            self.patience_counter = 0
        
        return True
```

### 8. Data Loading Issues

#### ğŸš¨ ì¦ìƒ
```
DataLoaderê°€ ë©ˆì¶¤
ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°œìƒ
ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨
```

#### âœ… í•´ê²° ë°©ë²•

**ì•ˆì „í•œ ë°ì´í„° ë¡œë”**
```python
class SafeDataset(Dataset):
    """ì•ˆì „í•œ ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    
    def __init__(self, data, tokenizer, max_length=512):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # ë°ì´í„° ê²€ì¦
        self.validated_data = self._validate_data()
    
    def _validate_data(self):
        """ë°ì´í„° ê²€ì¦ ë° ì •ì œ"""
        validated = []
        
        for i, item in enumerate(self.data):
            try:
                # í…ìŠ¤íŠ¸ ì¡´ì¬ í™•ì¸
                if 'text' not in item or not item['text']:
                    print(f"âš ï¸ ì¸ë±ìŠ¤ {i}: í…ìŠ¤íŠ¸ ì—†ìŒ")
                    continue
                
                # ê¸¸ì´ í™•ì¸
                if len(item['text']) < 10:
                    print(f"âš ï¸ ì¸ë±ìŠ¤ {i}: í…ìŠ¤íŠ¸ ë„ˆë¬´ ì§§ìŒ")
                    continue
                
                # ì¸ì½”ë”© í…ŒìŠ¤íŠ¸
                encoded = self.tokenizer.encode(item['text'])
                if len(encoded) > self.max_length:
                    # ìë¥´ê¸°
                    item['text'] = self.tokenizer.decode(
                        encoded[:self.max_length-1]
                    )
                
                validated.append(item)
                
            except Exception as e:
                print(f"âŒ ì¸ë±ìŠ¤ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"âœ… ë°ì´í„° ê²€ì¦ ì™„ë£Œ: {len(validated)}/{len(self.data)}")
        return validated
    
    def __len__(self):
        return len(self.validated_data)
    
    def __getitem__(self, idx):
        try:
            item = self.validated_data[idx]
            
            # í† í°í™”
            encoding = self.tokenizer(
                item['text'],
                truncation=True,
                padding='max_length',
                max_length=self.max_length,
                return_tensors='pt'
            )
            
            return {
                'input_ids': encoding['input_ids'].squeeze(),
                'attention_mask': encoding['attention_mask'].squeeze(),
                'labels': encoding['input_ids'].squeeze()
            }
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {idx}): {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                'input_ids': torch.zeros(self.max_length, dtype=torch.long),
                'attention_mask': torch.zeros(self.max_length, dtype=torch.long),
                'labels': torch.zeros(self.max_length, dtype=torch.long)
            }

# ì•ˆì „í•œ ë°ì´í„° ë¡œë” ìƒì„±
def create_safe_dataloader(dataset, batch_size=2):
    """ì•ˆì „í•œ ë°ì´í„° ë¡œë” ìƒì„±"""
    
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,          # MPS í˜¸í™˜
        pin_memory=False,       # MPS í˜¸í™˜
        drop_last=True,         # ë¶ˆì™„ì „í•œ ë°°ì¹˜ ì œê±°
        collate_fn=None         # ê¸°ë³¸ collate
    )
```

## ğŸŸ¢ Low Priority Issues

### 9. Performance Optimization

#### ğŸ“Š ì„±ëŠ¥ ì¸¡ì •
```python
import time
import psutil
import torch

class PerformanceProfiler:
    """ì„±ëŠ¥ í”„ë¡œíŒŒì¼ëŸ¬"""
    
    def __init__(self):
        self.start_time = None
        self.start_memory = None
        self.metrics = {}
    
    def start_profiling(self, task_name):
        """í”„ë¡œíŒŒì¼ë§ ì‹œì‘"""
        self.task_name = task_name
        self.start_time = time.time()
        
        if torch.backends.mps.is_available():
            self.start_memory = torch.mps.current_allocated_memory()
        
        print(f"ğŸš€ {task_name} í”„ë¡œíŒŒì¼ë§ ì‹œì‘")
    
    def end_profiling(self):
        """í”„ë¡œíŒŒì¼ë§ ì¢…ë£Œ"""
        end_time = time.time()
        duration = end_time - self.start_time
        
        if torch.backends.mps.is_available():
            end_memory = torch.mps.current_allocated_memory()
            memory_used = (end_memory - self.start_memory) / 1024**3
        else:
            memory_used = 0
        
        self.metrics[self.task_name] = {
            'duration': duration,
            'memory_used_gb': memory_used,
            'cpu_percent': psutil.cpu_percent(),
        }
        
        print(f"â±ï¸ {self.task_name} ì™„ë£Œ:")
        print(f"  ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")
        print(f"  ë©”ëª¨ë¦¬ ì‚¬ìš©: {memory_used:.2f}GB")
        print(f"  CPU ì‚¬ìš©ë¥ : {psutil.cpu_percent()}%")
    
    def get_report(self):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        return self.metrics

# ì‚¬ìš©ë²•
profiler = PerformanceProfiler()

profiler.start_profiling("ëª¨ë¸ ë¡œë”©")
model, tokenizer = safe_model_loading("Qwen/Qwen2.5-0.5B-Instruct")
profiler.end_profiling()

profiler.start_profiling("ë°ì´í„° ì „ì²˜ë¦¬")
dataset = SafeDataset(data, tokenizer)
profiler.end_profiling()

report = profiler.get_report()
```

### 10. Logging and Monitoring

#### ğŸ“ ì¢…í•© ë¡œê¹… ì‹œìŠ¤í…œ
```python
import logging
import json
from datetime import datetime

class LoopAILogger:
    """Loop AI ì „ìš© ë¡œê±°"""
    
    def __init__(self, log_file="loop_ai.log"):
        self.logger = logging.getLogger("LoopAI")
        self.logger.setLevel(logging.DEBUG)
        
        # íŒŒì¼ í•¸ë“¤ëŸ¬
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.DEBUG)
        
        # ì½˜ì†” í•¸ë“¤ëŸ¬
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # í¬ë§·í„°
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
    
    def log_training_start(self, config):
        """í•™ìŠµ ì‹œì‘ ë¡œê·¸"""
        self.logger.info("ğŸš€ í•™ìŠµ ì‹œì‘")
        self.logger.info(f"ì„¤ì •: {json.dumps(config, indent=2)}")
    
    def log_training_step(self, step, loss, lr):
        """í•™ìŠµ ìŠ¤í… ë¡œê·¸"""
        self.logger.debug(f"Step {step}: Loss={loss:.4f}, LR={lr:.2e}")
    
    def log_error(self, error, context=""):
        """ì—ëŸ¬ ë¡œê·¸"""
        self.logger.error(f"âŒ ì—ëŸ¬ ë°œìƒ {context}: {str(error)}")
    
    def log_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê·¸"""
        if torch.backends.mps.is_available():
            allocated = torch.mps.current_allocated_memory() / 1024**3
            self.logger.info(f"ğŸ MPS ë©”ëª¨ë¦¬: {allocated:.2f}GB")
    
    def log_model_save(self, path):
        """ëª¨ë¸ ì €ì¥ ë¡œê·¸"""
        self.logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {path}")
    
    def log_training_complete(self, final_loss, duration):
        """í•™ìŠµ ì™„ë£Œ ë¡œê·¸"""
        self.logger.info(f"ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        self.logger.info(f"ìµœì¢… Loss: {final_loss:.4f}")
        self.logger.info(f"ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")

# ì „ì—­ ë¡œê±° ì¸ìŠ¤í„´ìŠ¤
loop_logger = LoopAILogger()
```

## ğŸ”§ Automated Recovery Scripts

### ìë™ ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸
```python
#!/usr/bin/env python3
"""
Loop AI ìë™ ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸
ë¬¸ì œ ë°œìƒì‹œ ìë™ìœ¼ë¡œ ì§„ë‹¨í•˜ê³  ë³µêµ¬ ì‹œë„
"""

import sys
import subprocess
import torch
import importlib
from pathlib import Path

class AutoRecovery:
    """ìë™ ë³µêµ¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.recovery_log = []
    
    def diagnose_system(self):
        """ì‹œìŠ¤í…œ ì§„ë‹¨"""
        issues = []
        
        # 1. Python ë²„ì „ ì²´í¬
        if sys.version_info < (3, 8):
            issues.append("python_version")
        
        # 2. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì²´í¬
        required_packages = [
            'torch', 'transformers', 'peft', 'accelerate'
        ]
        
        for package in required_packages:
            try:
                importlib.import_module(package)
            except ImportError:
                issues.append(f"missing_{package}")
        
        # 3. MPS ì‚¬ìš© ê°€ëŠ¥ì„± ì²´í¬
        if not torch.backends.mps.is_available():
            issues.append("mps_unavailable")
        
        # 4. ë©”ëª¨ë¦¬ ì²´í¬
        if torch.backends.mps.is_available():
            try:
                # í…ŒìŠ¤íŠ¸ í…ì„œ ìƒì„±
                test_tensor = torch.randn(1000, 1000, device='mps')
                del test_tensor
                torch.mps.empty_cache()
            except RuntimeError:
                issues.append("mps_memory_issue")
        
        return issues
    
    def auto_fix(self, issues):
        """ìë™ ìˆ˜ì •"""
        
        for issue in issues:
            print(f"ğŸ”§ ìˆ˜ì • ì¤‘: {issue}")
            
            if issue.startswith("missing_"):
                package = issue.replace("missing_", "")
                self.install_package(package)
            
            elif issue == "mps_memory_issue":
                self.fix_memory_issue()
            
            elif issue == "python_version":
                print("âš ï¸ Python ë²„ì „ ì—…ê·¸ë ˆì´ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                print("í˜„ì¬: Python 3.8+ í•„ìš”")
        
        self.recovery_log.append(f"ìˆ˜ì • ì™„ë£Œ: {len(issues)}ê°œ ì´ìŠˆ")
    
    def install_package(self, package):
        """íŒ¨í‚¤ì§€ ì„¤ì¹˜"""
        try:
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", package
            ])
            print(f"âœ… {package} ì„¤ì¹˜ ì™„ë£Œ")
        except subprocess.CalledProcessError:
            print(f"âŒ {package} ì„¤ì¹˜ ì‹¤íŒ¨")
    
    def fix_memory_issue(self):
        """ë©”ëª¨ë¦¬ ë¬¸ì œ ìˆ˜ì •"""
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
            print("âœ… MPS ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
    
    def run_recovery(self):
        """ë³µêµ¬ ì‹¤í–‰"""
        print("ğŸš¨ Loop AI ìë™ ë³µêµ¬ ì‹œì‘")
        
        # ì§„ë‹¨
        issues = self.diagnose_system()
        
        if not issues:
            print("âœ… ì‹œìŠ¤í…œ ì •ìƒ")
            return True
        
        print(f"ğŸ” ë°œê²¬ëœ ë¬¸ì œ: {len(issues)}ê°œ")
        for issue in issues:
            print(f"  - {issue}")
        
        # ìë™ ìˆ˜ì •
        self.auto_fix(issues)
        
        # ì¬ì§„ë‹¨
        remaining_issues = self.diagnose_system()
        
        if not remaining_issues:
            print("ğŸ‰ ëª¨ë“  ë¬¸ì œ í•´ê²° ì™„ë£Œ!")
            return True
        else:
            print(f"âš ï¸ ë‚¨ì€ ë¬¸ì œ: {len(remaining_issues)}ê°œ")
            return False

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    recovery = AutoRecovery()
    success = recovery.run_recovery()
    
    if not success:
        print("ğŸ†˜ ìˆ˜ë™ ê°œì…ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì—¬ ìˆ˜ë™ìœ¼ë¡œ í•´ê²°í•´ì£¼ì„¸ìš”.")
    
    sys.exit(0 if success else 1)
```

## ğŸ“š Emergency Contacts & Resources

### ë¹ ë¥¸ ì°¸ì¡°
```python
# ì‘ê¸‰ ëª…ë ¹ì–´ë“¤
EMERGENCY_COMMANDS = {
    "ë©”ëª¨ë¦¬ ì •ë¦¬": "torch.mps.empty_cache(); gc.collect()",
    "ëª¨ë¸ ì–¸ë¡œë“œ": "del model; del tokenizer",
    "ê°€ìƒí™˜ê²½ ì¬ì‹œì‘": "deactivate && source venv/bin/activate",
    "íŒ¨í‚¤ì§€ ì¬ì„¤ì¹˜": "pip install --force-reinstall transformers",
    "ë¡œê·¸ í™•ì¸": "tail -f loop_ai.log",
}

# ìœ ìš©í•œ ë§í¬ë“¤
USEFUL_LINKS = {
    "Transformers ë¬¸ì„œ": "https://huggingface.co/docs/transformers",
    "PEFT ë¬¸ì„œ": "https://huggingface.co/docs/peft",
    "PyTorch MPS": "https://pytorch.org/docs/stable/notes/mps.html",
    "Qwen ëª¨ë¸": "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct",
}

# ì²´í¬ë¦¬ìŠ¤íŠ¸
TROUBLESHOOTING_CHECKLIST = [
    "âœ… ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸",
    "âœ… í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸", 
    "âœ… MPS ì‚¬ìš© ê°€ëŠ¥ í™•ì¸",
    "âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸",
    "âœ… ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸",
    "âœ… ë°ì´í„° í˜•ì‹ í™•ì¸",
    "âœ… íƒ€ì… íŒíŠ¸ ì •í™•ì„± í™•ì¸",
    "âœ… ë¡œê·¸ íŒŒì¸ í™•ì¸"
]
```

---

**Document Version**: 1.0  
**Last Updated**: 2025-01-27  
**Authors**: Loop AI Research Team  
**Status**: Complete Troubleshooting Guide

---

*ì´ ë¬¸ì„œëŠ” Loop AI í”„ë¡œì íŠ¸ì—ì„œ ì‹¤ì œë¡œ ë°œìƒí•œ ëª¨ë“  ë¬¸ì œì™€ í•´ê²° ê³¼ì •ì„ ê¸°ë¡í•œ ì‹¤ì „ ë§¤ë‰´ì–¼ì…ë‹ˆë‹¤. í–¥í›„ ìœ ì‚¬í•œ ë¬¸ì œ ë°œìƒì‹œ ì¦‰ì‹œ ì°¸ì¡°í•  ìˆ˜ ìˆë„ë¡ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.* 