#!/usr/bin/env python3
"""
Qwen2.5-0.5B Model Download Script
=================================

ê¸°ê°€ì°¨ë“œ ëª¨ë“œë¡œ ìµœì‹  Qwen2.5-0.5B ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•œë‹¤.
Apple Silicon MPS ìµœì í™”ë¥¼ ê³ ë ¤í•œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ê²€ì¦.
"""

import os
import sys
import time
import logging
from pathlib import Path
from typing import Optional, List
import argparse

try:
    from huggingface_hub import snapshot_download, hf_hub_download
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    import psutil
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}")
    print("ğŸ’¡ ì„¤ì¹˜ ëª…ë ¹ì–´: pip install transformers torch huggingface_hub psutil")
    sys.exit(1)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('model_download.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class QwenModelDownloader:
    """Qwen2.5 ëª¨ë¸ ë‹¤ìš´ë¡œë” - ê¸°ê°€ì°¨ë“œ ë²„ì „"""
    
    def __init__(self, cache_dir: str = "./models"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ì§€ì› ëª¨ë¸ ëª©ë¡ (ì„±ëŠ¥ ìˆœì„œ)
        self.supported_models = {
            "qwen2.5-0.5b": "Qwen/Qwen2.5-0.5B",
            "qwen2.5-0.5b-instruct": "Qwen/Qwen2.5-0.5B-Instruct", 
            "qwen2-0.5b": "Qwen/Qwen2-0.5B",
            "qwen2-0.5b-instruct": "Qwen/Qwen2-0.5B-Instruct"
        }
        
        # Apple Silicon ìµœì í™” ì„¤ì •
        self.device_info = self._detect_device()
        
    def _detect_device(self) -> dict:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ê°ì§€"""
        info = {
            "device": "cpu",
            "device_name": "CPU",
            "memory_gb": psutil.virtual_memory().total / (1024**3),
            "mps_available": False
        }
        
        if torch.cuda.is_available():
            info["device"] = "cuda"
            info["device_name"] = torch.cuda.get_device_name(0)
            info["memory_gb"] = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            info["device"] = "mps"
            info["device_name"] = "Apple Silicon MPS"
            info["mps_available"] = True
            
        return info
    
    def download_model(self, model_key: str, force_download: bool = False) -> str:
        """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        if model_key not in self.supported_models:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_key}")
            
        model_id = self.supported_models[model_key]
        local_dir = self.cache_dir / model_key
        
        logger.info(f"ğŸš€ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {model_id}")
        logger.info(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {local_dir}")
        logger.info(f"ğŸ’» ë””ë°”ì´ìŠ¤: {self.device_info['device_name']}")
        
        if local_dir.exists() and not force_download:
            logger.info(f"âœ… ëª¨ë¸ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {local_dir}")
            return str(local_dir)
        
        try:
            start_time = time.time()
            
            # ë³‘ë ¬ ë‹¤ìš´ë¡œë“œë¡œ ì†ë„ ìµœì í™”
            snapshot_download(
                repo_id=model_id,
                local_dir=str(local_dir),
                cache_dir=str(self.cache_dir / "hub"),
                resume_download=True,
                max_workers=8,  # ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ
                local_files_only=False
            )
            
            download_time = time.time() - start_time
            logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {download_time:.2f}ì´ˆ")
            
            return str(local_dir)
            
        except Exception as e:
            logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def verify_model(self, model_path: str) -> bool:
        """ëª¨ë¸ ê²€ì¦"""
        logger.info(f"ğŸ” ëª¨ë¸ ê²€ì¦ ì¤‘: {model_path}")
        
        try:
            # í† í¬ë‚˜ì´ì € ë¡œë“œ í…ŒìŠ¤íŠ¸
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            logger.info(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ - ì–´íœ˜ í¬ê¸°: {len(tokenizer)}")
            
            # ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ (í—¤ë”ë§Œ)
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16 if self.device_info["device"] == "cuda" else torch.float32,
                device_map=None,  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ë¡œë“œí•˜ì§€ ì•ŠìŒ
                low_cpu_mem_usage=True
            )
            
            # ê¸°ë³¸ ì •ë³´ ì¶œë ¥
            num_params = sum(p.numel() for p in model.parameters())
            logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ - íŒŒë¼ë¯¸í„° ìˆ˜: {num_params:,}")
            
            # ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
            if self.device_info["mps_available"]:
                device = torch.device("mps")
            elif torch.cuda.is_available():
                device = torch.device("cuda")
            else:
                device = torch.device("cpu")
                
            model = model.to(device)
            
            # í•œêµ­ì–´ í…ŒìŠ¤íŠ¸
            test_text = "ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ”"
            inputs = tokenizer(test_text, return_tensors="pt").to(device)
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=10,
                    do_sample=False,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            result = tokenizer.decode(outputs[0], skip_special_tokens=True)
            logger.info(f"âœ… ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì„±ê³µ: '{result}'")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def get_model_info(self, model_path: str) -> dict:
        """ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘"""
        info = {
            "path": model_path,
            "size_gb": 0,
            "files": [],
            "config": {}
        }
        
        model_dir = Path(model_path)
        if not model_dir.exists():
            return info
            
        # ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚°
        total_size = sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file())
        info["size_gb"] = total_size / (1024**3)
        
        # íŒŒì¼ ëª©ë¡
        info["files"] = [f.name for f in model_dir.iterdir() if f.is_file()]
        
        # ì„¤ì • íŒŒì¼ ì½ê¸°
        config_file = model_dir / "config.json"
        if config_file.exists():
            import json
            with open(config_file, 'r') as f:
                info["config"] = json.load(f)
        
        return info

def main():
    parser = argparse.ArgumentParser(description="Qwen2.5 ëª¨ë¸ ë‹¤ìš´ë¡œë”")
    parser.add_argument(
        "--model", 
        choices=["qwen2.5-0.5b", "qwen2.5-0.5b-instruct", "qwen2-0.5b", "qwen2-0.5b-instruct"],
        default="qwen2.5-0.5b-instruct",
        help="ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ì„ íƒ"
    )
    parser.add_argument("--cache-dir", default="./models", help="ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--force", action="store_true", help="ê°•ì œ ì¬ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--verify", action="store_true", help="ë‹¤ìš´ë¡œë“œ í›„ ê²€ì¦")
    
    args = parser.parse_args()
    
    print("ğŸ”¥ ê¸°ê°€ì°¨ë“œ ëª¨ë“œ - Qwen2.5 ëª¨ë¸ ë‹¤ìš´ë¡œë”")
    print("=" * 50)
    
    downloader = QwenModelDownloader(cache_dir=args.cache_dir)
    
    try:
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        model_path = downloader.download_model(args.model, force_download=args.force)
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        info = downloader.get_model_info(model_path)
        print(f"\nğŸ“Š ëª¨ë¸ ì •ë³´:")
        print(f"   ê²½ë¡œ: {info['path']}")
        print(f"   í¬ê¸°: {info['size_gb']:.2f} GB")
        print(f"   íŒŒì¼ ìˆ˜: {len(info['files'])}")
        
        if info["config"]:
            print(f"   ëª¨ë¸ íƒ€ì…: {info['config'].get('model_type', 'unknown')}")
            print(f"   ì–´íœ˜ í¬ê¸°: {info['config'].get('vocab_size', 'unknown')}")
            print(f"   ë ˆì´ì–´ ìˆ˜: {info['config'].get('num_hidden_layers', 'unknown')}")
        
        # ê²€ì¦ ì‹¤í–‰
        if args.verify:
            print(f"\nğŸ” ëª¨ë¸ ê²€ì¦ ì¤‘...")
            if downloader.verify_model(model_path):
                print("âœ… ëª¨ë¸ ê²€ì¦ ì™„ë£Œ!")
            else:
                print("âŒ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨!")
                sys.exit(1)
        
        print(f"\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print(f"ğŸ“ ëª¨ë¸ ê²½ë¡œ: {model_path}")
        
    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 