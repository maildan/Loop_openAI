#!/usr/bin/env python3
"""
NIA í•œêµ­ì–´ ì‚¬ì „ ë³€í™˜ê¸°
NIADic.xlsxë¥¼ JSON í˜•íƒœë¡œ ë³€í™˜í•˜ê³  ë§ì¶¤ë²• ê²€ì‚¬ìš© ìµœì í™”ëœ ì‚¬ì „ ìƒì„±
"""

import json
import os
import re
from typing import Dict, List, Set, Any, Optional

import pandas as pd
from pandas import DataFrame, Series

def load_nia_dictionary(file_path: str) -> Optional[DataFrame]:
    """NIA ì‚¬ì „ Excel íŒŒì¼ ë¡œë”©"""
    try:
        print(f"ğŸ“– NIA ì‚¬ì „ ë¡œë”© ì¤‘: {file_path}")
        
        # Excel íŒŒì¼ ì½ê¸° (ì²« ë²ˆì§¸ ì‹œíŠ¸)
        df = pd.read_excel(file_path, sheet_name=0)
        print(f"âœ… ë¡œë”© ì™„ë£Œ: {len(df):,}ê°œ í–‰")
        
        # ì»¬ëŸ¼ëª… í™•ì¸
        print(f"ğŸ“‹ ì»¬ëŸ¼: {list(df.columns)}")
        return df
        
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None

def clean_word(word: Any) -> Optional[str]:
    """ë‹¨ì–´ ì •ë¦¬ ë° ê²€ì¦"""
    if pd.isna(word):
        return None
    
    # ë¬¸ìì—´ë¡œ ë³€í™˜
    word_str = str(word).strip()
    
    if not word_str:
        return None
    
    # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ í—ˆìš© (íŠ¹ìˆ˜ë¬¸ì ì œê±°)
    cleaned = re.sub(r'[^\wê°€-í£]', '', word_str)
    
    # ìµœì†Œ ê¸¸ì´ í™•ì¸
    if len(cleaned) < 2:
        return None
    
    return cleaned

def process_nia_data(df: DataFrame) -> Dict[str, Any]:
    """NIA ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„"""
    print("ğŸ” ë°ì´í„° ë¶„ì„ ì¤‘...")
    
    # ì»¬ëŸ¼ëª… ë§¤í•‘ (ì‹¤ì œ ì»¬ëŸ¼ëª…ì— ë§ê²Œ ì¡°ì •)
    word_columns = []
    for col in df.columns:
        col_lower = str(col).lower()
        if any(keyword in col_lower for keyword in ['ì–´íœ˜', 'ë‹¨ì–´', 'word', 'í‘œì œì–´', 'ì–´ê·¼']):
            word_columns.append(col)
    
    print(f"ğŸ“ ì‹ë³„ëœ ë‹¨ì–´ ì»¬ëŸ¼: {word_columns}")
    
    # ëª¨ë“  ë‹¨ì–´ ìˆ˜ì§‘
    all_words: Set[str] = set()
    word_stats: Dict[str, int] = {}
    
    for col in word_columns:
        if col in df.columns:
            print(f"ğŸ“Š {col} ì»¬ëŸ¼ ì²˜ë¦¬ ì¤‘...")
            
            # ê° ì»¬ëŸ¼ì˜ ê³ ìœ ê°’ ê°œìˆ˜
            try:
                unique_count = int(df[col].nunique())
                word_stats[col] = unique_count
            except (TypeError, ValueError):
                word_stats[col] = 0
            print(f"   - ê³ ìœ ê°’: {unique_count:,}ê°œ")
            
            # ë‹¨ì–´ ì •ë¦¬ ë° ì¶”ê°€
            series_data = df[col].dropna()
            for word in series_data:
                cleaned = clean_word(word)
                if cleaned:
                    all_words.add(cleaned)
    
    print(f"âœ… ì´ ìˆ˜ì§‘ëœ ë‹¨ì–´: {len(all_words):,}ê°œ")
    
    return {
        "words": sorted(list(all_words)),
        "statistics": {
            "total_words": len(all_words),
            "column_stats": word_stats,
            "original_rows": len(df)
        }
    }

def create_optimized_spellcheck_dict(words: List[str], max_words: int = 50000) -> Dict[str, Any]:
    """ë§ì¶¤ë²• ê²€ì‚¬ìš© ìµœì í™”ëœ ì‚¬ì „ ìƒì„±"""
    print(f"âš¡ ë§ì¶¤ë²• ê²€ì‚¬ìš© ì‚¬ì „ ìµœì í™” ì¤‘... (ìµœëŒ€ {max_words:,}ê°œ)")
    
    # ë‹¨ì–´ ê¸¸ì´ì™€ ì‚¬ìš© ë¹ˆë„ë¥¼ ê³ ë ¤í•œ ì ìˆ˜ ê³„ì‚°
    word_scores = []
    
    for word in words:
        score = 0
        
        # ê¸¸ì´ ì ìˆ˜ (2-10ìê°€ ìµœì )
        if 2 <= len(word) <= 10:
            score += 10
        elif len(word) > 10:
            score -= 2
        
        # í•œê¸€ ë‹¨ì–´ ìš°ì„ 
        if re.match(r'^[ê°€-í£]+$', word):
            score += 5
        
        # ì¼ë°˜ì ì¸ ì–´ë¯¸/ì ‘ì‚¬ ì œì™¸
        common_endings = ['í•˜ë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤', 'ì ', 'ì„±', 'í™”']
        if not any(word.endswith(ending) for ending in common_endings):
            score += 3
        
        word_scores.append((word, score))
    
    # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
    word_scores.sort(key=lambda x: x[1], reverse=True)
    
    # ìƒìœ„ ë‹¨ì–´ ì„ íƒ
    selected_words = [word for word, score in word_scores[:max_words]]
    
    print(f"âœ… ìµœì í™” ì™„ë£Œ: {len(selected_words):,}ê°œ ë‹¨ì–´ ì„ íƒ")
    
    return {
        "words": selected_words,
        "total_count": len(selected_words),
        "optimization_info": {
            "original_count": len(words),
            "selected_count": len(selected_words),
            "reduction_ratio": f"{(1 - len(selected_words)/len(words))*100:.1f}%"
        }
    }

def save_dictionaries(data: Dict[str, Any], output_dir: str = "dataset/words"):
    """ì‚¬ì „ íŒŒì¼ë“¤ ì €ì¥"""
    os.makedirs(output_dir, exist_ok=True)
    
    # ì „ì²´ ì‚¬ì „ ì €ì¥
    full_dict_path = os.path.join(output_dir, "nia_dictionary.json")
    print(f"ğŸ’¾ ì „ì²´ ì‚¬ì „ ì €ì¥ ì¤‘: {full_dict_path}")
    
    full_dict = {
        "metadata": {
            "source": "NIA í•œêµ­ì–´ ì‚¬ì „",
            "created_at": pd.Timestamp.now().isoformat(),
            "total_words": len(data["words"]),
            "statistics": data["statistics"]
        },
        "words": data["words"]
    }
    
    with open(full_dict_path, 'w', encoding='utf-8') as f:
        json.dump(full_dict, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ì „ì²´ ì‚¬ì „ ì €ì¥ ì™„ë£Œ: {os.path.getsize(full_dict_path) / 1024 / 1024:.1f}MB")
    
    # ìµœì í™”ëœ ë§ì¶¤ë²• ê²€ì‚¬ìš© ì‚¬ì „ ìƒì„±
    optimized_data = create_optimized_spellcheck_dict(data["words"])
    
    spell_dict_path = os.path.join(output_dir, "spellcheck_dictionary.json")
    print(f"ğŸ’¾ ë§ì¶¤ë²• ì‚¬ì „ ì €ì¥ ì¤‘: {spell_dict_path}")
    
    spell_dict = {
        "metadata": {
            "source": "NIA í•œêµ­ì–´ ì‚¬ì „ (ë§ì¶¤ë²• ê²€ì‚¬ ìµœì í™”)",
            "created_at": pd.Timestamp.now().isoformat(),
            "optimization_info": optimized_data["optimization_info"]
        },
        "words": optimized_data["words"]
    }
    
    with open(spell_dict_path, 'w', encoding='utf-8') as f:
        json.dump(spell_dict, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ë§ì¶¤ë²• ì‚¬ì „ ì €ì¥ ì™„ë£Œ: {os.path.getsize(spell_dict_path) / 1024 / 1024:.1f}MB")
    
    return full_dict_path, spell_dict_path

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ NIA í•œêµ­ì–´ ì‚¬ì „ ë³€í™˜ ì‹œì‘!")
    
    # íŒŒì¼ ê²½ë¡œ
    input_file = "dataset/words/NIADic.xlsx"
    
    if not os.path.exists(input_file):
        print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_file}")
        return
    
    # 1. ì‚¬ì „ ë¡œë”©
    df = load_nia_dictionary(input_file)
    if df is None:
        return
    
    # 2. ë°ì´í„° ì²˜ë¦¬
    processed_data = process_nia_data(df)
    
    # 3. ì‚¬ì „ íŒŒì¼ ì €ì¥
    full_path, spell_path = save_dictionaries(processed_data)
    
    print("\n" + "="*50)
    print("ğŸ‰ ë³€í™˜ ì™„ë£Œ!")
    print(f"ğŸ“š ì „ì²´ ì‚¬ì „: {full_path}")
    print(f"ğŸ” ë§ì¶¤ë²• ì‚¬ì „: {spell_path}")
    print(f"ğŸ“Š ì´ ë‹¨ì–´ ìˆ˜: {processed_data['statistics']['total_words']:,}ê°œ")
    print("="*50)

if __name__ == "__main__":
    main() 